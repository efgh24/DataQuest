{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ae8e6c",
   "metadata": {},
   "source": [
    "# Functional Programming: Map, Filter, Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa5baf",
   "metadata": {},
   "source": [
    "1. Write two functions:\n",
    "    * read(): takes in a filename, reads the file, then returns a list of lines from the file.\n",
    "    * count(): takes in a list, and returns its length.\n",
    "2. Call read() on the example_log.txt file, and assign the return value to a example_lines variable.\n",
    "3. Call count() on example_lines, and assign the return value to the lines_count variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b976fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.155.108.44 - - [30/Nov/2017:11:59:54 +0000] \"PUT /categories/categories/categories HTTP/1.1\" 401 963 \"http://www.yates.com/list/tags/category/\" \"Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332\"\n",
      "\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def read(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [line for line in f]\n",
    "    \n",
    "def count(lst):\n",
    "    return len(lst)\n",
    "\n",
    "example_lines = read(\"example_log.txt\")\n",
    "lines_count = count(example_lines)\n",
    "\n",
    "print(example_lines[0])\n",
    "print(lines_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5ca17",
   "metadata": {},
   "source": [
    "### Instead of the def syntax for function declaration, we can use a lambda expression to write Python functions. The lambda syntax closely follows the def syntax, but it's not a 1-to-1 mapping. Here's an example of building a function that adds two numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec76d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Using `def` (old way).\n",
    "def old_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Using `lambda` (new way).\n",
    "new_add = lambda a, b: a + b\n",
    "\n",
    "print(old_add(10, 5) == new_add(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544b071",
   "metadata": {},
   "source": [
    "The lambda expression takes in comma-separated sequences of inputs (like def). Then, immediately following the colon, it returns the expression without using an explicit return statement. Finally, when assigning the lambda expression to a variable, it acts exactly like a Python function, and we can call it using the function call syntax: new_add().\n",
    "\n",
    "If we didn't assign lambda to a variable name, it would be called an anonymous function. These anonymous functions are extremely helpful, especially when using them as an input for another function. For example, the sorted() function takes in an optional key argument (a function) that describes how the items in a list should be sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cf8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 0), ('c', 4), ('b', 6), ('a', 10)]\n"
     ]
    }
   ],
   "source": [
    "unsorted = [('b', 6), ('a', 10), ('d', 0), ('c', 4)]\n",
    "\n",
    "# Sort on the second tuple value (the integer).\n",
    "print(sorted(unsorted, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6eaf1",
   "metadata": {},
   "source": [
    "1. Call sorted on the lines variable and in the key argument sort on the following:\n",
    "    * Split the line on empty spaces ' '\n",
    "    * Return the 6th element on the split line\n",
    "2. Assign the sorted return value to a sorted_lines variable.\n",
    "3. Print the sorted_lines variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b7e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233.154.7.24 - - [30/Nov/2017:11:59:54 +0000] \"GET /app HTTP/1.1\" 404 526 \"http://www.cherry.com/main.htm\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5360 (KHTML, like Gecko) Chrome/13.0.839.0 Safari/5360\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [line for line in f]\n",
    "    \n",
    "lines = read('example_log.txt')\n",
    "\n",
    "sorted_lines = sorted(lines, key=lambda x: x.split(' ')[5])\n",
    "print(sorted_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe94b6",
   "metadata": {},
   "source": [
    "**First-class functions** - The ability to pass in functions as arguments isn't unique to Python.\n",
    "\n",
    "There are a set of important first-class functions that are common within the functional paradigm. These functions take in a Python iterable, and, like sorted, apply a function for each element in the list.\n",
    "\n",
    "The first function we'll work with is the map() function. The map() function takes in an iterable (i.e., list), and creates a new iterable object: a special map object. The first-class function applies to every element in the new object. \n",
    "\n",
    "Here's how we could use map() to add 10 or 20 to every element in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a0a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14, 15]\n",
      "[21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Note: We convert the returned map object to\n",
    "# a list data structure.\n",
    "add_10 = list(map(lambda x: x + 10, values))\n",
    "add_20 = list(map(lambda x: x + 20, values))\n",
    "\n",
    "print(add_10)\n",
    "print(add_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3fc15",
   "metadata": {},
   "source": [
    "It's important to cast the return value from map() as a list object. Using the returned map object is difficult to work with if you're expecting it to function like a list. Printing it doesn't show each of its items, and you can only iterate over it once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95373807",
   "metadata": {},
   "source": [
    "1. Map each line in the lines variable to its corresponding IP address:\n",
    "    * Split the line on empty spaces ' '\n",
    "    * Return the first element on the split line\n",
    "2. Cast the mapped object to a list, and assign it to the ip_addresses variable.\n",
    "3. Print the ip_addresses variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "481c250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200.155.108.44', '36.139.255.202', '50.112.115.219', '204.132.56.4', '233.154.7.24']\n"
     ]
    }
   ],
   "source": [
    "lines = read('example_log.txt')\n",
    "\n",
    "ip_addresses = list(map(lambda x: x.split(' ')[0], lines))\n",
    "\n",
    "print(ip_addresses[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44525e",
   "metadata": {},
   "source": [
    "The second function we'll work with is the filter() function. The filter() function takes in an iterable, creates a new iterable object (again, a special map object), and a first-class function that must return a bool value. The new map object is a filtered iterable of all the elements that returned True.\n",
    "\n",
    "Here's how we could filter odd or even values from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cdf542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "[1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Note: We convert the returned filter object to\n",
    "# a list data structure.\n",
    "even = list(filter(lambda x: x % 2 == 0, values))\n",
    "odd = list(filter(lambda x: x % 2 == 1, values))\n",
    "\n",
    "print(even)\n",
    "print(odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8ac1a",
   "metadata": {},
   "source": [
    "1. Filter each line in the ip_addresses list to IP addresses that begin with less than or equal to 20.\n",
    "2. Cast the filtered object to a list, and assign it to the filtered_ips variable.\n",
    "3. Print the filtered_ips variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b96e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '200.155.108.44'\n",
    "x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c7105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.31.18.29', '5.237.70.145', '4.186.143.85', '7.205.198.134', '2.98.108.99']\n"
     ]
    }
   ],
   "source": [
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "\n",
    "print(filtered_ips[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d21da",
   "metadata": {},
   "source": [
    "The last function we'll look at is the reduce() function from the functools package. The reduce() function takes in a function and an iterable object such as a list. It will then reduce the list to a single value by successively applying the given function. It will first apply it on the first two elements and replace them with the result. Then it will apply the function on the first result and the next element and so on until a single value remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c557504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "values = [1, 2, 3, 4]\n",
    "\n",
    "summed = reduce(lambda a, b: a + b, values)\n",
    "print(summed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922d052",
   "metadata": {},
   "source": [
    "Note that you do not have to operate on the second value in the lambda expression. For example, you can write a function that always returns the first value of an iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e05ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "# By convention, we add `_` as a placeholder for an input\n",
    "# we do not use.\n",
    "first_value = reduce(lambda a, _: a, values)\n",
    "print(first_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd792a",
   "metadata": {},
   "source": [
    "Another important aspect of reduce is that the lambda function doesn't necessarily need to return a value that is the same type as the inputs. Imagine that we have a list of words and that we want to use ```reduce()``` to add all word lengths together. For example, for the list ```[\"I\", \"love\", \"data\", \"science\"]``` the answer would be 1 + 4 + 4 + 7 = 16.\n",
    "\n",
    "The first solution that might come to mind is to use the lambda function ```lambda a, b: len(a) + len(b)``` that adds the lengths of two strings. The problem however is that after adding the length of \"I\" with \"love\", reduce() will apply the lambda function on that result, which is 5, and \"data\" as shows bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031aece",
   "metadata": {},
   "source": [
    "This will result in an error because ```len(5)``` isn't defined. To overcome this, we need to change the lambda function to account for two cases:\n",
    "\n",
    "1. Both a and b are strings\n",
    "2. a is already an integer, and b is a string\n",
    "\n",
    "The following code shows this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed5d31cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "total_len = reduce(lambda x, y: len(x) + len(y) if isinstance(x, str) else x + len(y), [\"I\", \"love\", \"data\", \"science\"])\n",
    "print(total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa15e4",
   "metadata": {},
   "source": [
    "To complete this exercise, you'll need to use both special cases that we described above: using a single of the argument and having two cases in the lambda function.\n",
    "\n",
    "1. Using ```reduce```, count the total number of elements in ```lines``` and ```filtered_ips```.\n",
    "2. Find the ratio between ```filtered_ips``` and ```lines```, and assign the value to ```ratio```.\n",
    "3. Print the ```ratio``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e3ccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0808\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "\n",
    "count_all = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, lines)\n",
    "count_filtered = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, filtered_ips)\n",
    "\n",
    "ratio = count_filtered / count_all\n",
    "\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf000f3",
   "metadata": {},
   "source": [
    "Because we eventually convert to lists, we should rewrite the ```map()``` and ```filter()``` functions using list comprehension instead. This is the more Pythonic way of writing them — we're taking advantage of the Python syntax for making lists. Here's how you could translate the previous examples of ```map()``` and ```filter()``` to list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd16c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Map.\n",
    "add_10 = [x + 10 for x in values]\n",
    "print(add_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d821722",
   "metadata": {},
   "source": [
    "From the examples, you can see that we don't need to add the lambda expressions. If you want to add ```map()```, or ```filter()``` functions to your code, this is usually the recommended way. However, on the next screen, we'll show you a case when you would still use the map() and filter() functions.\n",
    "\n",
    "1. Using list comprehension, do the following:\n",
    "    * Rewrite the ip_addresses mapping\n",
    "    * Rewrite the filtered_ips filter\n",
    "2. Keeping everything else, print the ratio variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10cae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0808\n"
     ]
    }
   ],
   "source": [
    "lines = read('example_log.txt')\n",
    "# Rewrite ip_addresses, and filtered_ips.\n",
    "# list(map(lambda x: x.split()[0], lines))\n",
    "\n",
    "ip_addresses = [line.split()[0] for line in lines]\n",
    "\n",
    "# list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "\n",
    "filtered_ips = [ip.split('.')[0] for ip in ip_addresses if int(ip.split('.')[0]) <= 20]\n",
    "\n",
    "count_all = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, lines)\n",
    "count_filtered = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, filtered_ips)\n",
    "ratio = count_filtered / count_all\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f471be6",
   "metadata": {},
   "source": [
    "Sometimes we want to use the behavior of a function, but we want to decrease the number of arguments it takes. The purpose is to \"save\" one of the inputs, and create a new function that defaults the behavior using the saved input. For example, perhaps we want to write a function that will always add 2 to any number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018d09b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "def add_two(b):\n",
    "    return 2 + b \n",
    "\n",
    "print(add_two(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e374d6d",
   "metadata": {},
   "source": [
    "The **add_two** function is similar to the general function, ```f(a,b)=a+b```, only it defaults one of the arguments ```(a=2)```. In Python, we can use the **partial module** from the **functools** package to set these argument defaults. The **partial** module takes in a function, and \"freezes\" any number of args (or kwargs), starting from the first argument, then returns a new function with the default inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69411f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "add_two = partial(add, 2)\n",
    "add_ten = partial(add, 10)\n",
    "\n",
    "print(add_two(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c56ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(add_ten(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039e242",
   "metadata": {},
   "source": [
    "Partials can take in any function, including functions from the standard library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f75c2c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['200.155.108.44',\n",
       " '36.139.255.202',\n",
       " '50.112.115.219',\n",
       " '204.132.56.4',\n",
       " '233.154.7.24']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A partial that grabs IP addresses using\n",
    "# the `map` function from the exercises.\n",
    "extract_ips = partial(\n",
    "    map,\n",
    "    lambda x: x.split(' ')[0]\n",
    ")\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(extract_ips(lines))\n",
    "ip_addresses[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e908376",
   "metadata": {},
   "source": [
    "1. Using a partial, create a count function that takes in a list and runs the reduce implementation of list counting.\n",
    "2. Replace the reduce with count for the following:\n",
    "    * count_all\n",
    "    * count_filtered\n",
    "3. Keeping everything else, print the ratio variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eccbbd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0808\n",
      "0.0808\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "\n",
    "# reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, lines)\n",
    "# reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, filtered_ips)\n",
    "\n",
    "ratio = count_filtered / count_all\n",
    "print(ratio)\n",
    "from functools import partial\n",
    "\n",
    "count = partial(\n",
    "    reduce,\n",
    "    lambda x, _: 2 if isinstance(x, str) else x + 1\n",
    ")\n",
    "\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = [line.split()[0] for line in lines]\n",
    "filtered_ips = [\n",
    "    ip.split('.')[0]\n",
    "    for ip in ip_addresses if int(ip.split('.')[0]) <= 20\n",
    "]\n",
    "count_all = count(lines)\n",
    "count_filtered =  count(filtered_ips)\n",
    "ratio = count_filtered / count_all\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36800f",
   "metadata": {},
   "source": [
    "Let's examine how we used each of the ```map```, ```filter```, and ```reduce``` functions. In the exercises, we first mapped a list of log lines to their IP addresses, filtered those IP addresses for IPs that start with an integer less than or equal to 20, then counted the results. Notice that the output of each function was inputted into the one immediately following it.\n",
    "\n",
    "Viewing our exercises this way, it's as if we've created a chain of function calls starting from ```map``` and ending with ```reduce```. This chain of function calls has a term in mathematics called function composition. Given a chain of functions, ```f(x), g(x), h(x)```, function composition occurs when you apply the output of each function to the input of the next: ```h(g(f(x)))```.\n",
    "\n",
    "This is exactly the same concept we used in our exercises:\n",
    "\n",
    "```reduce(filter(map(...)))```\n",
    "\n",
    "Using a function ```compose``` that takes in a sequence of **single argument** functions, we can create a composed single argument function similar to the example above. Here's a composed function with ```int``` types instead of iterable types:\n",
    "\n",
    "```\n",
    "def add_two(x):\n",
    "    return x + 2\n",
    "\n",
    "def multiply_by_four(x):\n",
    "    return x * 4\n",
    "\n",
    "def subtract_seven(x):\n",
    "    return x - 7\n",
    "\n",
    "composed = compose(\n",
    "    add_two,  # + 2\n",
    "    multiply_by_four,  # * 4\n",
    "    subtract_seven  # - 7\n",
    ")\n",
    "\n",
    "# (((10 + 2) * 4) - 7) = 41\n",
    "answer = composed(10)\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "By restricting each ```map```, ```filter```, and ```reduce``` functions, requiring only a single input (an iterable), we can rewrite our previous implementations as a composable function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9bae66",
   "metadata": {},
   "source": [
    "1. Using ```compose```, combine the ```map```, ```filter```, and ```reduce``` functions (with ```partial```) to create a composable function that takes in a list and returns a filtered count.\n",
    "2. Assign the result of the composed function to the variable ```counted```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b207ca55",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/4027948409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcounted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/compose.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrappers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from compose import compose\n",
    "\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "\n",
    "ratio = count_filtered / count_all\n",
    "extract_ips = partial(\n",
    "    map,\n",
    "    lambda x: x.split()[0]\n",
    ")\n",
    "filter_ips = partial(\n",
    "    filter,\n",
    "    lambda x: int(x.split('.')[0]) <= 20\n",
    ")\n",
    "count = partial(\n",
    "    reduce,\n",
    "    lambda x, _: 2 if isinstance(x, str) else x + 1\n",
    ")\n",
    "compose = compose(\n",
    "    extract_ips,\n",
    "    filter_ips,\n",
    "    count\n",
    ")\n",
    "counted = compose(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a398b",
   "metadata": {},
   "source": [
    "# Pipeline Tasks\n",
    "\n",
    "In the previous lesson, we learned about functional programming. We briefly spoke about the requirements of tasks, and how a combination of tasks combine to create a data pipeline. In this lesson, we will build on the functional programming concepts we learned, and construct a real pipeline from scratch.\n",
    "\n",
    "The goal of our pipeline will be to take log lines, from the ```example_log.txt``` file, and create a summary CSV file of unique HTTP request types and their associated counts. Each line from the ```example_log.txt``` file is from an NGINX log file. This log file contains each client request sent to a running web server in a client-server model.\n",
    "\n",
    "Here's the first few lines of the log:\n",
    "```\n",
    "200.155.108.44 - - [30/Nov/2017:11:59:54 +0000] \"PUT /categories/categories/categories HTTP/1.1\" 401 963 \"http://www.yates.com/list/tags/category/\" \"Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332\"\n",
    "36.139.255.202 - - [30/Nov/2017:11:59:54 +0000] \"PUT /search HTTP/1.1\" 404 171 \"https://www.butler.org/main/tag/category/home.php\" \"Mozilla/5.0 (Macintosh; PPC Mac OS X 10_5_0) AppleWebKit/5332 (KHTML, like Gecko) Chrome/15.0.813.0 Safari/5332\"\n",
    "50.112.115.219 - - [30/Nov/2017:11:59:54 +0000] \"POST /main/blog HTTP/1.1\" 404 743 \"http://deleon-bender.com/categories/category.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_5 rv:2.0; apn-IN) AppleWebKit/531.48.1 (KHTML, like Gecko) Version/4.0 Safari/531.48.1\"\n",
    "204.132.56.4 - - [30/Nov/2017:11:59:54 +0000] \"POST /list HTTP/1.1\" 404 761 \"http://smith.com/category.htm\" \"Opera/9.39.(Windows 98; Win 9x 4.90; mn-MN) Presto/2.9.163 Version/12.00\"\n",
    "233.154.7.24 - - [30/Nov/2017:11:59:54 +0000] \"GET /app HTTP/1.1\" 404 526 \"http://www.cherry.com/main.htm\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5360 (KHTML, like Gecko) Chrome/13.0.839.0 Safari/5360\"\n",
    "```\n",
    "\n",
    "Each line follows the convention:\n",
    "```\n",
    "$remote_addr - $remote_user [$time_local] \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_agent\"\n",
    "```\n",
    "\n",
    "* \\$remote_addr — the ip address of the client making the request to the server.\n",
    "* \\$remote_user — if the client authenticated with basic authentication, this is the user name (blank in the examples above).\n",
    "* \\$time_local — the local time when the request was made.\n",
    "* \\$request — the type of request, and the URL that it was made to.\n",
    "* \\$status — the response status code from the server.\n",
    "* \\$body_bytes_sent — the number of bytes sent by the server to the client in the response body.\n",
    "* \\$http_referrer — the page that the client was on before sending the current request.\n",
    "* \\$http_user_agent — information about the browser and system of the client.\n",
    "\n",
    "In our exercises, we will be focusing on the **request** type that can be one of POST, GET, or PUT. To begin, we're going to learn about special iterable types in Python, called generators. Then, we will use these generators to create a highly performant data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d672ee3",
   "metadata": {},
   "source": [
    "### Generators in Python\n",
    "\n",
    "Before we can dive into our task pipeline, we need to introduce **generators** in Python. The best way to do this is with an example.\n",
    "\n",
    "In the previous lesson, we would read in the ```example_log.txt``` file, and write it to a list. Recall that when creating a list, Python loads each element of the list into RAM. For files that exceed multiple gigabytes, this file loading can cause a program to run out of memory.\n",
    "\n",
    "Instead of reading the file into memory, we can take advantage of **file streaming**. File streaming works by breaking a file into small sections (called **chunks**), and then loaded one at time into memory. Once a chunk has been exhausted (all the bytes of that chunk has been read), Python requests the next chunk, and then that chunk is loaded into memory to be iterated on.\n",
    "\n",
    "This is abstracted away when you run the following:\n",
    "\n",
    "```\n",
    "with open('example_log.txt') as file:\n",
    "    for line in file:\n",
    "        # The file acts like an iterator.\n",
    "        print(line)\n",
    "```\n",
    "\n",
    "We can see evidence of exhausted bytes if you try to read from the opened file again:\n",
    "\n",
    "```\n",
    "with open('example_log.txt') as file:\n",
    "    for line in file:\n",
    "        do_something()\n",
    "\n",
    "    # At this point, the file has been read and\n",
    "    # no unread bytes are remaining.\n",
    "    for line in file:\n",
    "        # The `file` is empty and the loop ends\n",
    "        # immediately.\n",
    "        do_something()\n",
    "```\n",
    "\n",
    "This stream-like behavior is extremely helpful when working with large data sets. We can replicate this behavior with other iterators with the use of **generators**. A generator is an iterable object that is created from a **generator function**.\n",
    "\n",
    "The generator function differs from a regular function by two important differences:\n",
    "* A generator uses ```yield```, instead of ```return```. (However, a ```return``` statement is used to stop iteration, more on that soon).\n",
    "* Local variables are kept in memory until the generator completes.\n",
    "\n",
    "Here's a simple example of a generator that calculates the squares of each (non-negative) number up to and excluding ```N```:\n",
    "\n",
    "```\n",
    "def squares(N):\n",
    "    for i in range(N):\n",
    "        yield i * i\n",
    "\n",
    "for i in squares(4):\n",
    "    print(i)\n",
    "```\n",
    "\n",
    "Let's break down this snippet, highlighting the mentioned differences. First, notice that there is no ```return``` statement, but instead, there is a ```yield``` expression. The ```yield``` expression is responsible for two actions:\n",
    "\n",
    "* A signal to the Python interpreter that this function will be a generator.\n",
    "* Suspends the function execution, keeping the local variables in memory, until the next call.\n",
    "\n",
    "The suspension of execution, saving local variables, and then resuming operation is what allows the generator to act like a stream. We can even make generators that can continue execution forever. Here's a count generator that continuously counts from 1 until the program terminates:\n",
    "\n",
    "```\n",
    "def count():\n",
    "    i = 1\n",
    "    while True:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "for i in count():\n",
    "    print(i)\n",
    "```\n",
    "\n",
    "Using the ```next()``` function, you can see the generator and yield suspension work in action. In an iteration (like a ```for``` loop), the Python interpreter continuously calls the ```next()``` function to receive the \"next\" element in the iterable. In a generator, each call to the ```next()``` function completes a cycle, and then stops at the next ```yield```.\n",
    "\n",
    "```\n",
    "print(next(count()))\n",
    "1\n",
    "print(next(count()))\n",
    "2\n",
    "```\n",
    "\n",
    "\n",
    "Suppose we wanted to give an upper limit to the ```count()``` function. Then we need to use a ```return``` statement within the generator. The ```return``` statement is one way that a Python loop (eg. ```for```) knows when to stop looping. Using ```return``` without an argument ends the function and returns ```None```, breaking the loop. Here's how we would update ```count()``` using ```return```:\n",
    "\n",
    "```\n",
    "# Count with an upper limit of `N`.\n",
    "def count(N):\n",
    "    i = 1\n",
    "    while True\n",
    "        if i > N:\n",
    "            return\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "for i in count(5):\n",
    "    print(i)\n",
    "```\n",
    "\n",
    "* Rewrite the squares(N) example generator using a while True loop, instead of a for loop.\n",
    "    * Using a while loop is tricky, so if you hit an infinite loop, cancel the code run and check your logic.\n",
    "* Using the new squares(N) generator, create a list of squared elements for squares(20).\n",
    "* Assign the list to the variable squared_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8471a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361]\n"
     ]
    }
   ],
   "source": [
    "def squares(N):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i >= N:\n",
    "            return\n",
    "        yield i * i\n",
    "        i += 1\n",
    "\n",
    "squared_values = [i for i in squares(20)]\n",
    "print(squared_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729a8aa",
   "metadata": {},
   "source": [
    "### Pipeline Tasks\n",
    "\n",
    "Generator comprehensions are extremely similar to list comprehensions. We can turn any list comprehension into a generator comprehension by replacing the square brackets ```[]``` to parenthesis ```()```. For example, here's how we could write the ```squares``` function in the previous screen as a list and generator expression:\n",
    "\n",
    "```\n",
    "squared_list = [i * i for i in range(20)]\n",
    "squared_gen = (i * i for i in range(20))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Before you begin replacing all your lists as generators, let's discuss a major drawback of the generator. Suppose we had two places in our code that wanted to use the ```squared_gen``` generator. With a list, ```squared_list```, we could easily do:\n",
    "\n",
    "```\n",
    "num_to_square = {}\n",
    "for idx, i in enumerate(squared_list):\n",
    "    num_to_square[idx] = i\n",
    "print(num_to_square)\n",
    "```\n",
    "```\n",
    "{0: 0, 1: 1, 2: 4, 3: 9 ...}\n",
    "```\n",
    "```\n",
    "for i in squared_list:\n",
    "    print(i)\n",
    "```\n",
    "```\n",
    "0\n",
    "1\n",
    "4\n",
    "9\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "Using a generator, however, the second loop will **not** run. Like a file, a generator will exhaust all it's elements once the final yield has been executed. Be cautious of this behavior when using generators, like ```squared_gen``` in your code!\n",
    "\n",
    "```\n",
    "num_to_square = {}\n",
    "for idx, i in enumerate(squared_gen):\n",
    "    num_to_square[idx] = i\n",
    "print(num_to_square)\n",
    "```\n",
    "```\n",
    "{0: 0, 1: 1, 2: 4, 3: 9 ...}\n",
    "```\n",
    "```\n",
    "for i in squared_gen:\n",
    "    print(i)\n",
    "```\n",
    "```\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef78811",
   "metadata": {},
   "source": [
    "### Manipulating Generators in Tasks\n",
    "\n",
    "It's time to use generators in our pipeline. Recall from the previous lesson that we combined a sequence of maps, filters, reducers, and produced a final count output. Using a sequence of generators, instead of the built-in objects, we will mimic this compose behavior for our pipeline.\n",
    "\n",
    "To restate our goals for the lesson, we want to perform the following to get from a raw log file to a summarized CSV:\n",
    "\n",
    "1. Read in the ```example_log.txt``` file.\n",
    "2. Parse the log file into a series of rows.\n",
    "3. Format the rows into a CSV file.\n",
    "4. Run some analysis on the CSV file, and count unique visitors.\n",
    "5. Format the analysis to a summarized CSV file.\n",
    "\n",
    "Each step can be isolated as an individual task. Notice that they can also be written to take in an iterable-like object (file, generator, CSV lines, etc), and also output an iterable. This replicable behavior is highly valuable for consistent data pipelines.\n",
    "\n",
    "Furthermore, we still want to adhere to the general practices of functional programming. The tenets being: highly composable functions with a focus on function purity.\n",
    "\n",
    "To emphasize composability, we can create a general ```parse()``` function that takes in a log file, splits the lines, and then extracts the fields. In the next screen we will perform some data cleaning, but for now we just want to get the data into a generator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "264afc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = open('example_log.txt')\n",
    "def parse_log(log):\n",
    "    for line in log:\n",
    "        split_line = line.split()\n",
    "        remote_addr = split_line[0]\n",
    "        time_local = split_line[3] + \" \" + split_line[4]\n",
    "        request_type = split_line[5]\n",
    "        request_path = split_line[6]\n",
    "        status = split_line[8]\n",
    "        body_bytes_sent = split_line[9]\n",
    "        http_referrer = split_line[10]\n",
    "        http_user_agent = \" \".join(split_line[11:])\n",
    "        yield (\n",
    "            remote_addr, time_local, request_type, request_path,\n",
    "            status, body_bytes_sent, http_referrer, http_user_agent\n",
    "        )\n",
    "\n",
    "first_line = next(parse_log(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a48d9812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('200.155.108.44',\n",
       " '[30/Nov/2017:11:59:54 +0000]',\n",
       " '\"PUT',\n",
       " '/categories/categories/categories',\n",
       " '401',\n",
       " '963',\n",
       " '\"http://www.yates.com/list/tags/category/\"',\n",
       " '\"Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332\"')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e08e56",
   "metadata": {},
   "source": [
    "### Data Cleaning in Parse Log\n",
    "\n",
    "We can update the ```parse_log()``` function to also perform some data cleaning for us. Notice that we've stored a ```local_time``` field with square brackets, the HTTP status code is a string, and there's unnecessary double quotes (```\"```) around some fields.\n",
    "\n",
    "To help fix these, we have exposed a couple utility functions for you in the exercise:\n",
    "\n",
    "```\n",
    "def parse_time(time_str):\n",
    "    \"\"\"\n",
    "    Parses time in the format [30/Nov/2017:11:59:54 +0000]\n",
    "    to a datetime object.\n",
    "    \"\"\"\n",
    "    time_obj = datetime.strptime(time_str, '[%d/%b/%Y:%H:%M:%S %z]')\n",
    "    return time_obj\n",
    "\n",
    "def strip_quotes(s):\n",
    "    return s.replace('\"', '')\n",
    "```\n",
    "\n",
    "\n",
    "* Enhance the parse_log() function by cleaning the following:\n",
    "    * Set the time_local field to a datetime object.\n",
    "    * Strip the quotes off request_type, http_referrer, and http_user_agent.\n",
    "    * Parse the status and body_bytes_sent to int.\n",
    "* Read the example_log.txt file, and call parse_log() on the file.\n",
    "* Call next() on the generator, and assign the return value to the variable first_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "736b4986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('200.155.108.44', datetime.datetime(2017, 11, 30, 11, 59, 54, tzinfo=datetime.timezone.utc), 'PUT', '/categories/categories/categories', 401, 963, 'http://www.yates.com/list/tags/category/', 'Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332')\n"
     ]
    }
   ],
   "source": [
    "log = open('example_log.txt')\n",
    "from datetime import datetime \n",
    "\n",
    "def parse_time(time_str):\n",
    "    \"\"\"\n",
    "    Parses time in the format [30/Nov/2017:11:59:54 +0000]\n",
    "    to a datetime object.\n",
    "    \"\"\"\n",
    "    time_obj = datetime.strptime(time_str, '[%d/%b/%Y:%H:%M:%S %z]')\n",
    "    return time_obj\n",
    "\n",
    "def strip_quotes(s):\n",
    "    return s.replace('\"', '')\n",
    "\n",
    "def parse_log(log):\n",
    "    for line in log:\n",
    "        split_line = line.split()\n",
    "        remote_addr = split_line[0]\n",
    "        time_local = parse_time(split_line[3] + \" \" + split_line[4])\n",
    "        request_type = strip_quotes(split_line[5])        \n",
    "        request_path = split_line[6]\n",
    "        status = int(split_line[8])\n",
    "        body_bytes_sent = int(split_line[9])\n",
    "        http_referrer = strip_quotes(split_line[10])\n",
    "        http_user_agent = strip_quotes(\" \".join(split_line[11:]))\n",
    "        yield (\n",
    "            remote_addr, time_local, request_type, request_path,\n",
    "            status, body_bytes_sent, http_referrer, http_user_agent\n",
    "        )\n",
    "        \n",
    "first_line = next(parse_log(log))\n",
    "print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380b511",
   "metadata": {},
   "source": [
    "### Write to CSV\n",
    "\n",
    "After parsing our logs into a generator of tuples, it's now time to write a task, and save the rows to a CSV file. This keeps the data in a well known data storage structure that we can use in future tasks. In the next lesson, we will discuss the role of files in a data pipeline.\n",
    "\n",
    "A CSV is best understood when it has a set of header names for the columns, and the proper data types for its values. After parsing the logs, we have the proper data types, but we don't have the metadata of the column names. At the end of the exercise, we'll want to have the following output for our CSV file:\n",
    "\n",
    "```\n",
    "ip,time_local,request_type,request_path,status,bytes_sent,http_referrer,http_user_agent\n",
    "200.155.108.44,2017-11-30 11:59:54+00:00,PUT,/categories/categories/categories,401,963,http://www.yates.com/list/tags/category/,\"Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332\"\n",
    "36.139.255.202,2017-11-30 11:59:54+00:00,PUT,/search,404,171,https://www.butler.org/main/tag/category/home.php,\"Mozilla/5.0 (Macintosh; PPC Mac OS X 10_5_0) AppleWebKit/5332 (KHTML, like Gecko) Chrome/15.0.813.0 Safari/5332\"\n",
    "```\n",
    "\n",
    "We have worked with the Python csv module a few times in the data engineering track. Here's an example of how you can use the csv module to write to a file:\n",
    "\n",
    "```\n",
    "import csv\n",
    "\n",
    "rows = [('a', 'b', 'c'), ('a1', 'b1', 'c1')]\n",
    "\n",
    "# Open file with read and write permissions.\n",
    "file = open('example_file.csv', 'w+')\n",
    "writer = csv.writer(file, delimiter=',')\n",
    "writer.writerows(rows)\n",
    "\n",
    "# Go to the beginning of the file.\n",
    "file.seek(0)\n",
    "print(file.readlines())\n",
    "```\n",
    "```\n",
    "\"a,b,c\\na1,b1,c1\\n\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d50c0",
   "metadata": {},
   "source": [
    "* Write a function build_csv() that takes in a required argument, lines (the parsed rows), file and optional argument header.\n",
    "    * If there is a header argument, insert the header at the beginning of the parsed rows.\n",
    "    * Write the CSV to the given file.\n",
    "    * Return the file.\n",
    "* Open the file temporary.csv for reading and writing.\n",
    "* Call build_csv() with the following variables:\n",
    "    * parsed for lines.\n",
    "    * A list or tuple of the header names in the screen's example.\n",
    "    * The temporary.csv file.\n",
    "* Assign the build_csv() return value to the variable csv_file.\n",
    "* Call csv_file.readlines() and assign the return value to the variable contents.\n",
    "* Print the first 5 rows of contents using print().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42da4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ip,time_local,request_type,request_path,status,bytes_sent,http_referrer,http_user_agent\\n', '200.155.108.44,2017-11-30 11:59:54+00:00,PUT,/categories/categories/categories,401,963,http://www.yates.com/list/tags/category/,\"Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332\"\\n', '36.139.255.202,2017-11-30 11:59:54+00:00,PUT,/search,404,171,https://www.butler.org/main/tag/category/home.php,\"Mozilla/5.0 (Macintosh; PPC Mac OS X 10_5_0) AppleWebKit/5332 (KHTML, like Gecko) Chrome/15.0.813.0 Safari/5332\"\\n', '50.112.115.219,2017-11-30 11:59:54+00:00,POST,/main/blog,404,743,http://deleon-bender.com/categories/category.html,\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_5 rv:2.0; apn-IN) AppleWebKit/531.48.1 (KHTML, like Gecko) Version/4.0 Safari/531.48.1\"\\n', '204.132.56.4,2017-11-30 11:59:54+00:00,POST,/list,404,761,http://smith.com/category.htm,Opera/9.39.(Windows 98; Win 9x 4.90; mn-MN) Presto/2.9.163 Version/12.00\\n']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "\n",
    "def build_csv(lines, file, header=None):\n",
    "    if header:\n",
    "        lines = [header] + [l for l in lines]\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "contents = csv_file.readlines()\n",
    "print(contents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f2f8e",
   "metadata": {},
   "source": [
    "### Chaining Iterators\n",
    "\n",
    "Unfortunately, if we wanted to append the **header** to the list of rows, then we had to convert those rows from a generator to list. By converting, we're losing the benefit of streaming the rows, and therefore there would be no point in creating a generator in the first place. If we want to keep the generator behavior, and insert a header, then we should use the **itertools.chain()** function to combine the two iterables.\n",
    "\n",
    "The **itertools.chain()** function combines a list of iterables together to create a single iterable object that runs through every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6009c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "a\n",
      "b\n",
      "0.48207631504821125\n",
      "0.8093692433537565\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "nums = [1, 2]\n",
    "letters = ('a', 'b')\n",
    "# Random number generator.\n",
    "randoms = (random.random() for _ in range(2))\n",
    "\n",
    "for ele in itertools.chain(nums, letters, randoms):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "299a792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ip,time_local,request_type,request_path,status,bytes_sent,http_referrer,http_user_agent\\n', '200.155.108.44,2017-11-30 11:59:54+00:00,PUT,/categories/categories/categories,401,963,http://www.yates.com/list/tags/category/,\"Mozilla/5.0 (Windows CE) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.864.0 Safari/5332\"\\n', '36.139.255.202,2017-11-30 11:59:54+00:00,PUT,/search,404,171,https://www.butler.org/main/tag/category/home.php,\"Mozilla/5.0 (Macintosh; PPC Mac OS X 10_5_0) AppleWebKit/5332 (KHTML, like Gecko) Chrome/15.0.813.0 Safari/5332\"\\n', '50.112.115.219,2017-11-30 11:59:54+00:00,POST,/main/blog,404,743,http://deleon-bender.com/categories/category.html,\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_5 rv:2.0; apn-IN) AppleWebKit/531.48.1 (KHTML, like Gecko) Version/4.0 Safari/531.48.1\"\\n', '204.132.56.4,2017-11-30 11:59:54+00:00,POST,/list,404,761,http://smith.com/category.htm,Opera/9.39.(Windows 98; Win 9x 4.90; mn-MN) Presto/2.9.163 Version/12.00\\n']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "\n",
    "def build_csv(lines, file, header=None):\n",
    "    # if header:\n",
    "    #    lines = [header] + [l for l in lines]\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "    \n",
    "contents = csv_file.readlines()\n",
    "print(contents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69c23e",
   "metadata": {},
   "source": [
    "### Counting Unique Request Types\n",
    "\n",
    "With the log file parsed and formatted to a CSV file, we can finally create the data summarization. Recall that the final step of the pipeline was to count the unique request types in the log file. In this exercise, we want to create a function that takes in the raw CSV file, and returns the following dict:\n",
    "\n",
    "```\n",
    "{\n",
    "    'GET': 3334,\n",
    "    'PUT': 3367,\n",
    "    'POST': 3299\n",
    "}\n",
    "```\n",
    "\n",
    "* Create a function called count_unique_requests() that takes in the CSV file, and returns the dict in the example.\n",
    "* Call count_unique_requests() on csv_file and assign the return value to the variable uniques.\n",
    "* Print the variable uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5bfdf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PUT': 3367, 'POST': 3299, 'GET': 3334}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "\n",
    "def count_unique_request(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('request_type')\n",
    "    \n",
    "    uniques = {}\n",
    "    for line in reader:\n",
    "        if not uniques.get(line[idx]):\n",
    "            uniques[line[idx]] = 0\n",
    "        uniques[line[idx]] += 1\n",
    "    return uniques\n",
    "\n",
    "uniques = count_unique_request(csv_file)\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b50f00",
   "metadata": {},
   "source": [
    "### Task Reusability\n",
    "\n",
    "Instead of a dictionary, we want to keep the output of the summarize task consistent with the rest of the pipeline. That is, we want the summarized task to output a generator of tuples. After converting the summarize task to a generator, we can reuse the build_csv() function to create a summarized CSV file like the following:\n",
    "\n",
    "```\n",
    "request_type,count\n",
    "GET,3334\n",
    "PUT,3367\n",
    "POST,3299\n",
    "```\n",
    "\n",
    "* Change the return value of count_unique_requests() to be a generator of key, value tuples from the dictionary.\n",
    "* Call build_csv() on the return value of count_unique_requests() with the header from the example and the file summarized.csv.\n",
    "* Print the return value of .read() from the opened summarized.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a35a6259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['request_type,count\\n', 'PUT,3367\\n', 'POST,3299\\n', 'GET,3334\\n']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def count_unique_request(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('request_type')\n",
    "    \n",
    "    uniques = {}\n",
    "    for line in reader:\n",
    "        \n",
    "        if not uniques.get(line[idx]):\n",
    "            uniques[line[idx]] = 0\n",
    "        uniques[line[idx]] += 1\n",
    "    return uniques\n",
    "\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "uniques = count_unique_request(csv_file)\n",
    "def count_unique_request(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('request_type')\n",
    "    \n",
    "    uniques = {}\n",
    "    for line in reader:\n",
    "        \n",
    "        if not uniques.get(line[idx]):\n",
    "            uniques[line[idx]] = 0\n",
    "        uniques[line[idx]] += 1\n",
    "    return ((k, v) for k,v in uniques.items())\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "uniques = count_unique_request(csv_file)\n",
    "summarized_file = open('summarized.csv', 'r+')\n",
    "summarized_csv = build_csv(uniques, summarized_file, header=['request_type', 'count'])\n",
    "print(summarized_file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46675146",
   "metadata": {},
   "source": [
    "In this lesson we expanded on the concept of functional programming, and explored how composition naturally creates a data pipeline. We built a sequence of tasks, and completed a pipeline that transformed raw log data into summarized CSV file.\n",
    "\n",
    "In the next lesson, we will generalize these tasks, and create a general purpose pipeline. We will learn about closures and function decorators that provide additional code reusability in functional programming. Finally, we will rebuild this pipeline using the general purpose pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45e723",
   "metadata": {},
   "source": [
    "# Building a Pipeline Class\n",
    "\n",
    "Let's take a second and think about what we've accomplished so far. First, we've learned about the concepts of functional programming, and how to write Python code using this paradigm. Next, we built a sequence of tasks that transformed a raw log file into a summarized CSV file.\n",
    "\n",
    "The problem with the sequence of tasks is that they were written **statically**. That is, they were written for one specific purpose, and we had to declare a variable for each function call. This process is difficult to extend, meaning we're required to rewrite our pipeline process each time we want to add, or change new tasks.\n",
    "\n",
    "Instead, we want to use a general purpose pipeline that makes it easy to build tasks and dependencies. Rather than calling a function, assigning the return value to an output variable, and then passing that variable to another function, we can use a general pipeline that works for all cases.\n",
    "\n",
    "By the end of this lesson, you will learn to do all the above using your own Pipeline class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76dbef13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/1764497092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfirst_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second_task)\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "\n",
    "print(pipeline.run(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939e838",
   "metadata": {},
   "source": [
    "This lesson will focus on introducing the concepts behind the **@pipeline.task()** syntax, how to combine object oriented design with the functional paradigm, and how to recreate the previous lesson's pipeline of tasks. To begin, we will start by introducing the last functional concept we'll need for our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110e687",
   "metadata": {},
   "source": [
    "### Inner Functions\n",
    "\n",
    "In the previous lesson, we wrote a ```build_csv()``` function that took in a Python file object, and then wrote out the file as a CSV format. A better way to write this is to accept either a file object, or a string filename. The benefit is that we can dynamically open a file, if given a filename, or use the given file object.\n",
    "\n",
    "We can write this behavior using an **inner function**. An inner function is, non-surprisingly, a function within a function. Here's what this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c7c8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [[1,2,3], [4,5,6]]\n",
    "\n",
    "def build_csv(lines, header=None, file=None):\n",
    "    def open_file(f):\n",
    "        # If it's a string, then open the file\n",
    "        # and return the opened file.\n",
    "        if isinstance(f, str):\n",
    "            f = open(f, 'w')\n",
    "        return f\n",
    "\n",
    "    file = open_file(file)  # add inner function.\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "csv_file = build_csv(lines, file='example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4700ea4",
   "metadata": {},
   "source": [
    "The benefit of these inner functions is that they are **encapsulated** in the scope of the parent function. You cannot call ```open_file()``` outside of the ```build_csv()``` function! Since we will only use ```open_file()``` within ```build_csv()```, this keeps the function isolated from the global scope of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f7733",
   "metadata": {},
   "source": [
    "### Function Closures\n",
    "\n",
    "The previous example introduced the concept of inner functions, and the benefit of encapsulation, but did not introduce the real benefits of inner functions. Before we start, recall the example of using ```partial``` for an ```add()``` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3190de05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "add_two = partial(add, 2)\n",
    "print(add_two(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f477d",
   "metadata": {},
   "source": [
    "The partial function returns a new function using the default ```a=2``` argument. We can replicate this behavior using an inner function instead. Let's see how this could work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12c0f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def add(A):\n",
    "    def inner(b):\n",
    "        # Use `A` from the argument in \n",
    "        # the parent add() function.\n",
    "        return A + b\n",
    "    return inner\n",
    "\n",
    "add_two = add(2)\n",
    "print(add_two(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57d44f",
   "metadata": {},
   "source": [
    "There are two key points to notice in this new ```add()``` function.\n",
    "\n",
    "    * ```inner()``` uses the ```A``` variable from the parent ```add()``` function.\n",
    "    * ```add()``` returns the ```inner()``` function.\n",
    "\n",
    "Let's break down these two points. The first shows that the inner function has access to the parent's scope. The second, because of a function's first-class status, it's possible to return the ```inner()``` function itself!\n",
    "\n",
    "Finally, notice that when we call the ```add()``` function with 2, the ```inner()``` function is returned to the ```add_two``` variable **with** the saved parent variable, *A=2*. That is, when we call the ```add_two()``` function, we are actually calling the ```inner()``` function with saved parent variables. It means that when the ```add()``` function is called, the ```inner()``` function is saved in working memory, ready to be called with those default values.\n",
    "\n",
    "These type of functions are called **closures**. A closure is defined by an inner function that has access to its parent's scope (ie. its variables). In Python, we can pass any amount of arguments from the parent function down to the inner function using the * symbol. Here's how we could create a ```add()``` function that added any arbitrary number of arguments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac7b8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# `*args` assigns a variable args to a tuple of\n",
    "# function arguments.\n",
    "# *args => args = (arg1, arg2, arg3, ...)\n",
    "def add(*args):\n",
    "    parent_args = args\n",
    "    def inner(*inner_args):\n",
    "        return sum(parent_args + inner_args)\n",
    "    return inner\n",
    "\n",
    "add_nine = add(1, 3, 5)\n",
    "print(add_nine(2, 4, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f68000",
   "metadata": {},
   "source": [
    "* Using a closure, recreate the partial() function:\n",
    "    * The first argument should be a function.\n",
    "    * The second argument should allow for any arbitrary amount of arguments.\n",
    "* Using the partial() function you wrote, call partial() on the add() function, and pass in 2.\n",
    "    * Assign the returned function to the variable add_two.\n",
    "* Call print() on add_two(7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00ee8380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def partial(func,*args):\n",
    "    parent_args =args\n",
    "    def inner(*inner_args):\n",
    "        return func(*(parent_args + inner_args))\n",
    "    return inner\n",
    "add_two = partial(add,2)\n",
    "print(add_two(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcd1d9",
   "metadata": {},
   "source": [
    "### Python Decorators\n",
    "\n",
    "In Python, it's sometimes convenient to log function calls for debugging purposes. Using a closure, it's possible to construct logging for any function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f94dc782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function: add\n",
      "With args: (1, 2)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def logger(func):\n",
    "    def inner(*args):\n",
    "        print('Calling function: {}'.format(func.__name__))\n",
    "        print('With args: {}'.format(args))\n",
    "        return func(*args)\n",
    "    return inner\n",
    "\n",
    "logged_add = logger(add)\n",
    "print(logged_add(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1640fe",
   "metadata": {},
   "source": [
    "The problem with our implementation, is that it would be tedious having to create a new ```logged_*``` variable for every function we wanted to log. Instead, Python provides **syntactic sugar**, (ie. a better way of writing syntax) to make this process easier. Using the **decorator** syntax (```@```), we can rewrite the example from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2abbbf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function: add\n",
      "With args: (1, 2)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def logger(func):\n",
    "    def inner(*args):\n",
    "        print('Calling function: {}'.format(func.__name__))\n",
    "        print('With args: {}'.format(args))\n",
    "        return func(*args)\n",
    "    return inner\n",
    "\n",
    "@logger\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "print(add(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043bb52",
   "metadata": {},
   "source": [
    "Notice that we didn't change anything in the **logger()** function, we just applied some new Python syntax. The syntax creates a decorator, which is a Python callable object (e.g. function), that modifies the behavior of any function, method, or class. By wrapping the **add()** function with the **@logger** decorator, we're telling the Python interpretor to call the **logger()** function for each invocation of **add()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1332e5a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2366091941.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/2366091941.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    add(1, 2) -> logger(add)(1, 2)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@logger\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Wrapping `add()` with `@logger`:\n",
    "add(1, 2) -> logger(add)(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340f36c",
   "metadata": {},
   "source": [
    "* Write a decorator function named catch_error() that does the following:\n",
    "    * Tries to run and return a given function.\n",
    "    * If any exception is thrown, catches the exception, and returns the exception object.\n",
    "* Add the decorator to the throws_error() function.\n",
    "* Run and print the function call of throws_error()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13ad3622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throws Error\n"
     ]
    }
   ],
   "source": [
    "# @catch_error\n",
    "def throws_error():\n",
    "    raise Exception('Throws Error')\n",
    "\n",
    "def catch_error(func):\n",
    "    def inner(*args):\n",
    "        try:\n",
    "            return func(*args)\n",
    "        except Exception as e:\n",
    "            return e\n",
    "    return inner\n",
    "\n",
    "@catch_error\n",
    "def throws_error():\n",
    "    raise Exception('Throws Error')\n",
    "    \n",
    "print(throws_error())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca6d91",
   "metadata": {},
   "source": [
    "### Method Decorators\n",
    "\n",
    "Recall that we are trying to create a pipeline class which dynamically adds tasks to be run. Rather than using functions as decorators, we can also use instance methods! Here's the behavior we're aiming for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f1686c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/2227123943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfirst_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "print(pipeline.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d8d52",
   "metadata": {},
   "source": [
    "Notice a couple things here:\n",
    "* The @pipeline.task() decorator is constructed from a class method.\n",
    "* The @pipeline.task() decorator is called before wrapping.\n",
    "* The first_task() function is being dynamically added to a list in the object.\n",
    "\n",
    "\n",
    "* Add a task() decorator method to the given Pipeline class that dynamically adds functions to the self.tasks list.\n",
    "* Instantiate a Pipeline object, and assign it to the variable pipeline.\n",
    "* Wrap the first_task() function with the @pipeline.task() decorator.\n",
    "* Print the pipeline.tasks property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "658be56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function first_task at 0x109e9b790>]\n"
     ]
    }
   ],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self):\n",
    "        def inner(f):\n",
    "            self.tasks.append(f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "    \n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "print(pipeline.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf27fe",
   "metadata": {},
   "source": [
    "### Decorator Arguments\n",
    "\n",
    "If we can call a decorator, before wrapping, then it's possible to pass in optional arguments before wrapping. Let's return to the behavior we are trying to build towards:\n",
    "\n",
    "```\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "print(pipeline.tasks)\n",
    "```\n",
    "\n",
    "We want the tasks to come in an ordered format. This ordering is based on the execution of our tasks, where we want the pipeline to run starting from the **first_task()** function, then executing the **second_task()**. The **depends_on** keyword argument enforces this ordering, so we can determine the dependency link of each task.\n",
    "\n",
    "\n",
    "* Add the depends_on optional keyword argument to the Pipeline.task() method.\n",
    "    * If the task depends on a function, then task() should search through the task list, and add the new task right after it.\n",
    "* Decorate the second_task() and last_task() functions with pipeline.task():\n",
    "    * second_task() depends on the first_task().\n",
    "    * last_task() depends on the second_task().\n",
    "* Print the pipeline.tasks property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72433d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function first_task at 0x109e9bb80>, <function second_task at 0x109e9b9d0>, <function last_task at 0x109e9b940>]\n"
     ]
    }
   ],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self):\n",
    "        def inner(f):\n",
    "            self.tasks.append(f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "    \n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "    \n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second_task)\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "\n",
    "print(pipeline.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60bb8b",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "\n",
    "With the tasks being inserted in order, it's now possible to write the pipeline's ```run()``` method. This method should act like functional composition where it takes the previous function's output, and inputs it into the following function. The behavior should execute like so:\n",
    "\n",
    "```\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second_task)\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "\n",
    "pipeline.run(20)\n",
    "# (((20 + 1) * 2) - 4) = 38\n",
    "```\n",
    "\n",
    "Notice that we're only inputing, and outputing, a single argument and value for each task.\n",
    "\n",
    "\n",
    "* Add the run() method to the Pipeline class.\n",
    "    * The run() method should take in an input_ argument.\n",
    "    * Then, it should iterate through the self.tasks property, and call each function with the previous output.\n",
    "* Call pipeline.run(20) and print() the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c5c6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "    \n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second_task)\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self, input_):\n",
    "        output = input_\n",
    "        for task in self.tasks:\n",
    "            output = task(output)\n",
    "        return output\n",
    "    \n",
    "pipeline = Pipeline()\n",
    "    \n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second_task)\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "\n",
    "print(pipeline.run(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d160307",
   "metadata": {},
   "source": [
    "### Challenge: Making Static Tasks Dynamic\n",
    "\n",
    "With the **pipeline.run()** method, we now have a fully functioning general pipeline! We can use this **Pipeline** class with any tasks that require a dependency ordering. Our previous lessons task pipeline fits this use case.\n",
    "\n",
    "Using the functions we wrote in the previous lesson, we will rebuild the pipeline using the **Pipeline** class we wrote this lesson. Here's the functions you wrote in the previous lesson, and the sequence of executing the tasks:\n",
    "\n",
    "```\n",
    "def parse_log(log):\n",
    "    for line in log:\n",
    "        split_line = line.split()\n",
    "        remote_addr = split_line[0]\n",
    "        time_local = parse_time(split_line[3] + \" \" + split_line[4])\n",
    "        request_type = strip_quotes(split_line[5])\n",
    "        request_path = split_line[6]\n",
    "        status = split_line[8]\n",
    "        body_bytes_sent = split_line[9]\n",
    "        http_referrer = strip_quotes(split_line[10])\n",
    "        http_user_agent = strip_quotes(\" \".join(split_line[11:]))\n",
    "        yield (\n",
    "            remote_addr, time_local, request_type, request_path,\n",
    "            status, body_bytes_sent, http_referrer, http_user_agent\n",
    "        )\n",
    "\n",
    "def build_csv(lines, header=None, file=None):\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "def count_unique_request(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('request_type')\n",
    "\n",
    "    uniques = {}\n",
    "    for line in reader:\n",
    "\n",
    "        if not uniques.get(line[idx]):\n",
    "            uniques[line[idx]] = 0\n",
    "        uniques[line[idx]] += 1\n",
    "    return ((k, v) for k,v in uniques.items())\n",
    "\n",
    "# Run the static tasks.\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ],\n",
    "    file=file\n",
    ")\n",
    "uniques = count_unique_request(csv_file)\n",
    "summarized_file = open('summarized.csv', 'r+')\n",
    "summarized_csv = build_csv(uniques, header=['request_type', 'count'], file=summarized_file)\n",
    "```\n",
    "\n",
    "Before we conclude, there's a helpful module that you can optionally use for file reading and writing. Instead of having to open a file, you can use the StringIO object from the io module. The StringIO object mimicks a file-like object that, instead of writing out to disk, keeps a file-like object in memory.\n",
    "\n",
    "```\n",
    "import io\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ],\n",
    "    # Using file-like object instead of `temporary.csv`.\n",
    "    file=io.StringIO()\n",
    ")\n",
    "uniques = count_unique_request(csv_file)\n",
    "summarized_csv = build_csv(\n",
    "    uniques,\n",
    "    header=['request_type', 'count'],\n",
    "    # Using file-like object instead of `summarized.csv`.\n",
    "    file=io.StringIO()\n",
    ")\n",
    "print(summarized_csv.readlines())\n",
    "```\n",
    "\n",
    "The **io.StringIO** object makes it easier to pass throughout the pipeline, and it includes the same API as a file. In the following exercise, we'll ask to use **io.StringIO**.\n",
    "\n",
    "\n",
    "* Rebuild the previous lesson's pipeline using the Pipeline class.\n",
    "    * The functions from the example code are provided to you in the exercise.\n",
    "    * The pipeline doesn't have to be identical. You can choose to omit, combine, or add on additional functions.\n",
    "    * Use io.StringIO() as the file object in the build_csv() keyword argument.\n",
    "* Call pipeline.run() on the log file.\n",
    "* Assign the return value to the variable summarized_csv, and print() the results of summarized_csv.readlines()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4ff694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['request_type,count\\r\\n', 'PUT,3367\\r\\n', 'POST,3299\\r\\n', 'GET,3334\\r\\n']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "    \n",
    "pipeline = Pipeline()\n",
    "log = open('example_log.txt')\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self, input_):\n",
    "        output = input_\n",
    "        for task in self.tasks:\n",
    "            output = task(output)\n",
    "        return output\n",
    "    \n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def parse_logs(logs):\n",
    "    return parse_log(logs)\n",
    "\n",
    "@pipeline.task(depends_on=parse_logs)\n",
    "def build_raw_csv(lines):\n",
    "    return build_csv(lines, header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ],\n",
    "    file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=build_raw_csv)\n",
    "def count_uniques(csv_file):\n",
    "    return count_unique_request(csv_file)\n",
    "\n",
    "@pipeline.task(depends_on=count_uniques)\n",
    "def summarize_csv(lines):\n",
    "    return build_csv(lines, header=['request_type', 'count'], file=io.StringIO())\n",
    "\n",
    "log = open('example_log.txt')\n",
    "summarized_file = pipeline.run(log)\n",
    "print(summarized_file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe1451",
   "metadata": {},
   "source": [
    "### Multiple Dependency Pipeline\n",
    "\n",
    "At the end of the previous lesson, we discussed some drawbacks with our initial pipeline implemention. One of the drawbacks was the restriction of only linear running tasks. Using our tasks as an example, we'll show why this is a major drawback that we must address.\n",
    "\n",
    "In our last lesson's task pipeline, the final task was to summarize logs that are outputted from a parsed CSV file. The summary is run on the request_type column name, but suppose that we wanted to also run a summarize on the status column. This seems doable – our only requirement should be the parsed CSV – but with our linear pipeline this will not work.\n",
    "\n",
    "Instead of a linear ordering, we need the ability to create multiple branches of dependencies. We're looking to build a data structure that can support the following task pipeline:\n",
    "\n",
    "This multiple branching pipeline is called a directed acyclic graph, or DAG for short. In this lesson, we will start by building our own DAG in Python, and then use the DAG to enhance our pipeline task scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283790e5",
   "metadata": {},
   "source": [
    "### Intro to DAGs\n",
    "\n",
    "In the introduction, we briefly mentioned the concept of a DAG. To describe the DAG, we're going to supplement the language of a graph with identical terms used in the course on trees. Let's break down what each of the terms mean:\n",
    "\n",
    "* Graph: The data structure is composed of vertices (nodes) and edges (branches).\n",
    "* Directed: Each edge of a vertex points only in one direction.\n",
    "* Acyclic: The graph does not have any cycles.\n",
    "\n",
    "The graph definition is ambigious, like the definition of a tree, so it's easier to describe with diagrams.\n",
    "\n",
    "The vertices (or nodes) are each point in the graph, and the edges are the lines that connect them. Note that there is no requirement of the direction of each edge (ie. The edge (2, 1) and (1, 2) are both possible). If we wanted to restrict the direction of an edge to only one possible combination, then it would be called a directed graph.\n",
    "\n",
    "The edge arrows correspond to the direction of the edges, which written out as a tuples, would be (1, 2), (2, 6), or (3, 5). If we follow a sequence of directed edges, like [(1, 3), (3, 5), (5, 7)], then we call this sequence a path of the graph.\n",
    "\n",
    "If there are any paths that starts, and ends with one vertex, then the graph contains a cycle. For example, if we had a path like [(1, 3), (3, 5), (5, 4), (4, 1)], then it would be a cycle. If a graph does not contain any cycles, then it is acyclic.\n",
    "\n",
    "The following graph has a cycle. In the exercise, we will ask you to find the cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6fb70",
   "metadata": {},
   "source": [
    "### The Dag Class\n",
    "\n",
    "Let's turn our attention back to the pipeline we wish to create:\n",
    "\n",
    "We can see from the diagram that the pipeline exhibits the requirements of a DAG. First, there are a set of vertices and edges, second, there is a direction of each task, and finally, there are no cycles. But just because the pipeline can be written as a DAG, why does that mean we should write is as such?\n",
    "\n",
    "The reason is that the DAG structure is built in a way that naturally creates an efficient ordering of dependent tasks. There is a DAG sorting algorithm for exposing this order that we can take advantage of when scheduling our tasks. We'll see that using a DAG, we can implement task scheduling in linear time, O(V+E)\n",
    ", where V and E are number of vertcies and edges).\n",
    "\n",
    "Before getting to deep into the weeds, let's begin by finding a way in Python to describe the DAG. First, notice that to create graph dependency, we could use a class like Vertex that contains a list of vertices. Like the root in a tree, we could build the DAG class with vertices like:\n",
    "\n",
    "```\n",
    "class DAG:\n",
    "    def __init__(self):\n",
    "        self.root = Vertex()\n",
    "\n",
    "class Vertex:\n",
    "    def __init__(self):\n",
    "        self.to = []\n",
    "        self.data = None\n",
    "\n",
    "dag = DAG()\n",
    "second = Vertex()\n",
    "dag.root.to.append(second)\n",
    "\n",
    "print(dag.root.to)\n",
    "```\n",
    "While this works, a graph structure does not always start with a single root. Instead, there could be multiple starting points in a DAG:\n",
    "\n",
    "Let's see if there's another data structure we could use. First, we need the ability to link vertices to multiple nodes in the graph. Then, we need to easily loop through these nodes to create our graph.\n",
    "\n",
    "A simple data structure which provides us with those each of those behaviors is a dict with list values. Here's how it could represent our example graph:\n",
    "\n",
    "```\n",
    "graph = {\n",
    "    # Node: List of nodes to.\n",
    "    7: [],\n",
    "    6: [7],\n",
    "    5: [7],\n",
    "    4: [7],\n",
    "    3: [5],\n",
    "    2: [6],\n",
    "    1: [2, 3, 4]\n",
    "}\n",
    "```\n",
    "\n",
    "Note: Thoughtout this lesson we use a DQ class which assists us answer-check our custom classes. If you remove this, your code can't pass our answer-checking mechanism.\n",
    "\n",
    "\n",
    "* Implement the add() method of the DAG class:\n",
    "    * Add node to self.graph if it isn't already in the graph and default the value to a list.\n",
    "    * If there is a to, add that to the node's list, and add to to self.graph defaulting to a list.\n",
    "* Run the code with the provided .add() test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "faa39503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG:\n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "    \n",
    "    def add(self, node, to=None):\n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if not to in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "\n",
    "dag = DAG()\n",
    "dag.add(1)\n",
    "dag.add(1, 2)\n",
    "dag.add(1, 3)\n",
    "dag.add(1, 4)\n",
    "dag.add(3, 5)\n",
    "dag.add(2, 6)\n",
    "dag.add(4, 7)\n",
    "dag.add(5, 7)\n",
    "dag.add(6, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f9433",
   "metadata": {},
   "source": [
    "### Sorting the DAG\n",
    "\n",
    "What does it mean to sort the DAG? In our previous sorting algorithms, we compared object values, and sorted them in ascending or descending order. In a DAG, though, what does it mean to be in ascending or descending order?\n",
    "\n",
    "Remember, our use case for the DAG was to place tasks in an order of dependencies. What we're looking for is an ordering of tasks that start with the most depended on tasks, and end with the least depended on. In our pipeline example, we're trying to order our tasks to start with parsing a file, and ending with summarizing.\n",
    "\n",
    "Let's take a look back at our DAG example. At a glance, it's reasonable to assume that the node that is depended on the most is the first node, 1. Then, following the paths, we can see that each node decreases in importance.\n",
    "\n",
    "Another way to phrase this, is that the longer the path to the node, the less that node is depended on. Take a look at 1, the most dependent, it has the smallest directed path: 0 steps. Conversely, the largest directed path is 7 with 4 steps, and it is the least depended on.\n",
    "\n",
    "Using this hypothesis, we can perform the following: 1. Find the \"root\" nodes of the graph with 0 dependecies. 2. For each node, find the longest path from the node to the roots. 3. Sort by the longest paths.\n",
    "\n",
    "This seems reasonable, but there's some major time complexity drawbacks here. For points 2, and 3, the time complexities of each one are O(n2) (longest path) and O(nlog n) (fastest sort), respectively! That's a worst case of O(n2).\n",
    "\n",
    "We mentioned that the we can find a sorting algorithm in linear time. In the next screen, we'll use the longer path hypothesis to create a linear time sorting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc471b",
   "metadata": {},
   "source": [
    "### Finding Number of In Degrees\n",
    "\n",
    "The previous screen's longest path hypothesis wasn't wrong, it was the proposed algorithm that would have taken too long. In the next few screens, we'll develop an efficient algorithm to sort the DAG.\n",
    "\n",
    "First, to find a longest path, it's necessary to know which nodes \"start\" the directed graph. By start, we mean the root nodes that the graph expands from. The question is, what determines a root node?\n",
    "\n",
    "Let's look at the following graph:\n",
    "\n",
    "Clearly, the root nodes of the graph are 1, 2, and 3. But what makes the root nodes different than every other node in the graph? The answer is, the number of in-degrees.\n",
    "\n",
    "The number of in-degrees is the total count of edges pointing towards the node. For example, the node 5 has 2 in-degrees, and the node 8 has 3. For each root node, however, the number of in-degrees will always be 0.\n",
    "\n",
    "\n",
    "* Note that we have created a BaseDAG class that contains the previous methods. It only contains the single add() method you wrote for the last exercise.\n",
    "* Implement the in_degrees() method:\n",
    "    * The in_degrees() method should create DAG.degrees attribute containing a dictionary mapping of node to number of in-degrees.\n",
    "    * Loop through every node, and its pointers, and then count each edge to the pointed node.\n",
    "* Run the code with the provided .in_degrees() tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0ca320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG():\n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "    def in_degrees(self):\n",
    "        self.degrees = {}\n",
    "        for node in self.graph:\n",
    "            if node not in self.degrees:\n",
    "                self.degrees[node] = 0\n",
    "            for pointed in self.graph[node]:\n",
    "                if pointed not in self.degrees:\n",
    "                    self.degrees[pointed] = 0\n",
    "                self.degrees[pointed] += 1\n",
    "    def add(self, node, to=None):\n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if not to in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "\n",
    "dag = DAG()\n",
    "dag.add(1)\n",
    "dag.add(1, 2)\n",
    "dag.add(1, 3)\n",
    "dag.add(1, 4)\n",
    "dag.add(3, 5)\n",
    "dag.add(2, 6)\n",
    "dag.add(4, 7)\n",
    "dag.add(5, 7)\n",
    "dag.add(6, 7)\n",
    "dag.in_degrees()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e2310",
   "metadata": {},
   "source": [
    "### Challenge: Sorting Dependencies\n",
    "\n",
    "With the in_degrees of each node, we can implement the next part of the algorithm: the walk. First, let's begin by filtering the root nodes. By the definition in the last screen, these are the nodes with in_degrees of 0.\n",
    "\n",
    "```\n",
    "root_nodes = []\n",
    "for node in in_degrees:\n",
    "    if in_degrees[node] == 0:\n",
    "        root_nodes.append(node)\n",
    "```\n",
    "\n",
    "Now that we have the root nodes, what do we do with them? Well, let's recall what the hypothesis is: the shorter the path, the more the node is depended on. Another way to state the hypothesis is: if there are root nodes, then those nodes are the most important nodes in the tree.\n",
    "\n",
    "If roots are the most important nodes, then what we should do is the following:\n",
    "\n",
    "\n",
    "1. Filter all the root nodes, and pop them off the graph.\n",
    "2. Search their pointers, and check if they are the new root nodes.\n",
    "    1. If one is, append it to the root nodes list, and pop it off the graph.\n",
    "    2. If not, then continue.\n",
    "3. Once all the nodes have been popped from the graph, return the list of ordered root nodes.\n",
    "\n",
    "This high level algorithm provides us with a systematic way to order the nodes in the correct order of importance. If we wanted to write the code solution, we could implement it using a queue (similar to tree traversals):\n",
    "\n",
    "\n",
    "1. Using in_degrees, place the root node(s) in a queue.\n",
    "2. While the queue is not empty:\n",
    "    1. Deque a node, node_i.\n",
    "    2. Check each of the pointers in the node.\n",
    "    3. Decrement the pointer's in_degrees by 1 (reduce pointers).\n",
    "    4. If that pointer's # of in-degrees is 0, add it to the queue.\n",
    "    5. If not, continue.\n",
    "    6. Once all the pointers have been searched, append node_i to the list searched.\n",
    "    7. Continue the while loop.\n",
    "3. Return the searched list.\n",
    "\n",
    "Using the description of the algorithm, we'll write the Python code for it in the DAG method sort(). This is a tricky algorithm, but using the methods we have already written, it's possible to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff6fc2a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseDAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/197932327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseDAG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_degrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseDAG' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DAG(BaseDAG):\n",
    "    def sort(self):\n",
    "        self.in_degrees()\n",
    "        to_visit = deque()\n",
    "        for node in self.graph:\n",
    "            if self.degrees[node] == 0:\n",
    "                to_visit.append(node)\n",
    "        \n",
    "        searched = []\n",
    "        while to_visit:\n",
    "            node = to_visit.popleft()\n",
    "            for pointer in self.graph[node]:\n",
    "                self.degrees[pointer] -= 1\n",
    "                if self.degrees[pointer] == 0:\n",
    "                    to_visit.append(pointer)\n",
    "            searched.append(node)\n",
    "        return searched\n",
    "\n",
    "dag = DAG()\n",
    "dag.add(1)\n",
    "dag.add(1, 2)\n",
    "dag.add(1, 3)\n",
    "dag.add(1, 4)\n",
    "dag.add(3, 5)\n",
    "dag.add(2, 6)\n",
    "dag.add(4, 7)\n",
    "dag.add(5, 7)\n",
    "dag.add(6, 7)\n",
    "dependencies = dag.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86cc12",
   "metadata": {},
   "source": [
    "### Enhance the Add Method\n",
    "\n",
    "The algorithm we wrote in the previous screen is called a topological sort. Specifically, the algorithm we implemented was called Kahn's Algorithm, a famous DAG sorting algorithm. An interesting property about this sorting algorithm is that it can also determine if a graph has a cycle or not.\n",
    "\n",
    "To test for cyclicity we first sort the DAG, return its toplogically sorted list of visited nodes, and then check the length of sorted nodes. If the length of the sorted nodes is greater than the length of the the nodes in the graph, then there must be a cycle! Because the topological sort visits all pointed nodes, if there is a cycle, we will be visiting a previous node making the visited list greater than the number of vertices in the graph.\n",
    "\n",
    "For robustness, we should not add a node to the DAG if it makes it cyclical.\n",
    "\n",
    "\n",
    "* Enhance the add() method to raise an error if a new node causes a cycle:\n",
    "    * Call sort() in the add() method, and check if the sorted length is greater than the number of nodes in the graph.\n",
    "    * Raise Exception if a cycle is detected.\n",
    "* Run the code with the provided .add() test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8c07ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseDAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/1988162248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseDAG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseDAG' is not defined"
     ]
    }
   ],
   "source": [
    "class DAG(BaseDAG):\n",
    "    def add(self, node, to=None):\n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if not to in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "            \n",
    "        if len(self.sort()) != len(self.graph):\n",
    "            raise Exception\n",
    "            \n",
    "dag = DAG()\n",
    "dag.add(1)\n",
    "dag.add(1, 2)\n",
    "dag.add(1, 3)\n",
    "dag.add(1, 4)\n",
    "dag.add(3, 5)\n",
    "dag.add(2, 6)\n",
    "dag.add(4, 7)\n",
    "dag.add(5, 7)\n",
    "dag.add(6, 7)\n",
    "# Add a pointer from 7 to 4, causing a cycle.\n",
    "dag.add(7, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee68fda",
   "metadata": {},
   "source": [
    "### Adding DAG to the Pipeline\n",
    "\n",
    "With a robust DAG, we now have the scheduler to build our intended pipeline. To start, let's add in the DAG class to the Pipeline. Instead of the tasks list property, we'll convert it to a DAG.\n",
    "\n",
    "\n",
    "* Rewrite the DAG.task method to add tasks to the self.tasks.graph.\n",
    "    * Make sure you're declaring the dependencies properly!\n",
    "* Add first(), second(), third(), and fourth() functions to the pipeline with @pipeline.task().\n",
    "    * second() depends on first().\n",
    "    * third() depends on second().\n",
    "    * fourth() depends on second().\n",
    "* Assign pipeline.tasks.graph to the variable graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed4932bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DQ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/3312071483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepends_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DQ' is not defined"
     ]
    }
   ],
   "source": [
    "class Pipeline(DQ):\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            pass\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "class Pipeline(DQ):\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "@pipeline.task()\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "@pipeline.task(depends_on=first)\n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "\n",
    "graph = pipeline.tasks.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8d28f",
   "metadata": {},
   "source": [
    "### Challenge: Running the Pipeline\n",
    "\n",
    "With the tasks added in order, it's time to run the pipeline. The major thing to notice is that there's no input for the run function. To run, we create a task that begins the pipeline by returning a static object (in our case, first() returns 20).\n",
    "\n",
    "Furthermore, there is no concept of a \"last task\" in a DAG. Therefore, the way we can represent our tasks, and their outputs during a run, is by using a dictionary that maps function: output. With this dictionary, we can store outputs after task completion so we can use them as inputs for the next tasks that require them.\n",
    "\n",
    "With that output dictionary, we can then choose to see the outputs of any function. This is the behavior we're looking for:\n",
    "\n",
    "```\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "@pipeline.task(depends_on=first)\n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "\n",
    "outputs = pipeline.run()\n",
    "print(outputs[third])\n",
    "```\n",
    "```\n",
    "print(outputs[fourth])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f16292",
   "metadata": {},
   "source": [
    "* Rewrite the DAG.run method:.\n",
    "    * Run the tasks.sort() method, and save it to a variable visited.\n",
    "    * Initialize a dictionary of completed.\n",
    "    * Iterate through each sorted task:\n",
    "        * Check every node in the graph, and if the task is referenced, run the task with the proper input and add it to completed.\n",
    "        * If the task is not referenced, run the task without arguments and add it to completed.\n",
    "    * Return the completed dictionary.\n",
    "* Run the pipeline with pipeline.run() and return the results to the variable outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2398c02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DQ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/srzmwfqn4sbcw3361mxwtgg80000gn/T/ipykernel_3168/3547287315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepends_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DQ' is not defined"
     ]
    }
   ],
   "source": [
    "class Pipeline(DQ):\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "@pipeline.task(depends_on=first)\n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "class Pipeline(DQ):\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self):\n",
    "        scheduled = self.tasks.sort()\n",
    "        completed = {}\n",
    "        \n",
    "        for task in scheduled:\n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values:\n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed:\n",
    "                completed[task] = task()\n",
    "        return completed\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "@pipeline.task(depends_on=first)\n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "@pipeline.task(depends_on=second)\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "\n",
    "outputs = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26492ca8",
   "metadata": {},
   "source": [
    "In this lesson, we solved the linear dependency mapping problem of our pipeline. First, we learned about a DAG, and how we can use it as a task scheduler. Then, we implemented the DAG, and added it to our pipeline.\n",
    "\n",
    "In the upcoming guided project, we'll use this new pipeline you built and run it on a real-life case. We'll learn about its drawbacks, and what changes need to be made. Finally, we'll introduce some thirdparty pipelines that we'll discuss in the upcoming courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28be10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
