{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb75361d",
   "metadata": {},
   "source": [
    "# The idea behind MapReduce is a generalization of what we implemented in the Process Pool Executors lesson:\n",
    "\n",
    "1. Divide: divide the data into chunks.\n",
    "2. Map: use parallel processing to process each chunk.\n",
    "3. Reduce: combine the individual chunk results into a global result.\n",
    "\n",
    "The goal of this lesson is to implement this workflow as a generic framework where a user provides some data, a map function, and a reduce function, and the data processes automatically. The parallel processing will occur during the map stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f05ba",
   "metadata": {},
   "source": [
    "## We provide you with the make_chunks function.\n",
    "\n",
    "1. Rename the df argument to data.\n",
    "\n",
    "2. On the first and second lines of the function, replace df.shape[0] with len(data).\n",
    "\n",
    "3. On the second line of the function, replace df[i:i+chunk_size] by data[i:i+chunk_size].\n",
    "\n",
    "4. Test the new implementation by calling it with arguments [1, 2, 3, 4, 5, 6] and 3. Assign the result to a variable named chunks.\n",
    "\n",
    "Inspect the chunks variable. Its value should be [[1, 2], [3, 4], [5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a4721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def make_chunks(data, num_chunks):\n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "chunks = make_chunks([1, 2, 3, 4, 5, 6],3)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987eb311",
   "metadata": {},
   "source": [
    "## Let's assume that we want to calculate the maximum value in a list of numbers using MapReduce. \n",
    "\n",
    "The first step is to use the make_chunks() function from the previous screen to break the list into smaller chunks. \n",
    "\n",
    "The next step is to implement a map function that, given a chunk, calculates the answer for that chunk. In this case, we can use the max() built-in function as our mapper function.\n",
    "\n",
    "\n",
    "\n",
    "### To make it easier to use, we can transform it into a function. The function has three arguments:\n",
    "\n",
    "1. mapper: the function that we want to apply to each chunk. In the example above, it would be the max() built-in function.\n",
    "\n",
    "2. data: the data.\n",
    "\n",
    "3. num_processes: the number of processes to use.\n",
    "\n",
    "The function implementation is the same as the code above, except that we replaced the max() built-in function with the provided mapper argument:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe001528",
   "metadata": {},
   "source": [
    "### We've provided you with the map_parallel() function and a list of numbers, values, that contains the same numbers as the diagram above.\n",
    "\n",
    "1. Call the map_parallel() function using the max() built-in function as mapper, the values list as data, and 5 processes. Assign the result to a variable named results.\n",
    "\n",
    "2. Inspect the value of results, and compare it with the diagram. It should be the same five numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cbb511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 41, 32, 35, 42]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def map_parallel(mapper, data, num_processes):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(mapper, chunk) for chunk in chunks]\n",
    "    return [future.result() for future in futures]\n",
    "\n",
    "values = [1, 4, 5, 2, 7, 21,     \\\n",
    "          31, 41, 3, 40, 5, 14,  \\\n",
    "          9, 32, 12, 18, 1, 30,  \\\n",
    "          6, 19, 23, 35, 12, 13, \\\n",
    "          0, 12, 42, 41, 11, 9]\n",
    "\n",
    "# Write code here\n",
    "results = map_parallel(max,values,5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0992dd",
   "metadata": {},
   "source": [
    "### We will learn a simpler way of doing this by using the Pool object from the multiprocessing module.\n",
    "\n",
    "We will learn a simpler way of doing this by using the **Pool object** from the multiprocessing module.\n",
    "\n",
    "We can use the **Pool** object to create and run a group of processes. We can initialize a **Pool** object by providing the number of processes that we want to run, like this:\n",
    "\n",
    "```\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(num_processes)\n",
    "```\n",
    "\n",
    "If the number of processes is not set, then it will automatically set to **os.cpu_count()**, which is the number of CPUs in the machine, like this:\n",
    "\n",
    "```\n",
    "import os\n",
    "print(os.cpu_count())\n",
    "```\n",
    "\n",
    "We can use the Pool.map() method to execute a function on chunks of data in parallel: \n",
    "1. The first argument is the function we want to apply. \n",
    "2. The second argument is an iterable with all function arguments. \n",
    "\n",
    "In our case, the second argument is a list that contains all chunks of data.\n",
    "\n",
    "Note that after calling the **Pool.map()** method, we call the **Pool.close()** and **Pool.join()** methods. The **Pool.close()** method prevents the addition of new processes to the pool. We need to execute this before we can join the processes. As before, the **Pool.join()** method makes the main program wait for all processes to finish before continuing executing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf83c8",
   "metadata": {},
   "source": [
    "We've provided you with the **values** list from the previous screen. We already broke it into six chunks using the **make_chunks()** function.\n",
    "\n",
    "1. Import the **Pool** object from the **multiprocessing** module.\n",
    "\n",
    "2. Use the **Pool()** constructor to build a pool with six processes. Assign the instance to a variable named **pool**.\n",
    "\n",
    "3. Use the **Pool.map()** method to apply the **max** function to **chunks**. Assign the result to a variable named **results**.\n",
    "\n",
    "4. Use the **Pool.close()** method to close the process pool.\n",
    "\n",
    "5. Use the **Pool.join()** method to wait for the processes to finish executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45359768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 41, 32, 30, 35, 42]\n"
     ]
    }
   ],
   "source": [
    "values = [1, 4, 5, 2, 7, 21,     \\\n",
    "          31, 41, 3, 40, 5, 14,  \\\n",
    "          9, 32, 12, 18, 1, 30,  \\\n",
    "          6, 19, 23, 35, 12, 13, \\\n",
    "          0, 12, 42, 41, 11, 9]\n",
    "\n",
    "chunks = make_chunks(values, 6)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(6)\n",
    "\n",
    "results = pool.map(max, chunks)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fb5c1",
   "metadata": {},
   "source": [
    "In reality, the **Pool.map()** function automatically blocks the execution until the processes finish their execution. So, we don't actually need to call the Pool.join() method. However, it is very important to call the Pool.close() method to destroy the processes after the execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9281d",
   "metadata": {},
   "source": [
    "### We've provided you with the values list from the previous screen. We already broke it into six chunks using the make_chunks() function.\n",
    "\n",
    "* Use a context manager (**with** statement) to create a **Pool** instance with six processes. Create it with the name **pool**.\n",
    "\n",
    "* Inside the context manager, use the **Pool.map()** method to apply the **max** function to **chunks**. Assign the result to a variable named **results**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06895138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 41, 32, 30, 35, 42]\n"
     ]
    }
   ],
   "source": [
    "values = [1, 4, 5, 2, 7, 21,     \\\n",
    "          31, 41, 3, 40, 5, 14,  \\\n",
    "          9, 32, 12, 18, 1, 30,  \\\n",
    "          6, 19, 23, 35, 12, 13, \\\n",
    "          0, 12, 42, 41, 11, 9]\n",
    "\n",
    "chunks = make_chunks(values, 6)\n",
    "\n",
    "# Write code here\n",
    "\n",
    "with Pool(6) as pool:\n",
    "    results = pool.map(max, chunks)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6d47f",
   "metadata": {},
   "source": [
    "### We need a way to combine the results of each chunk into a global result for the original data.\n",
    "\n",
    "For this, we can use the reduce function from the functools module. The functools.reduce() function takes two arguments:\n",
    "\n",
    "1. A reducer function.\n",
    "2. An iterable (a list for example) on which we want to apply the reducer function.\n",
    "\n",
    "The **functools.reduce()** function starts by applying the provided function to the first two elements of the iterable. Then it applies it to the first result and the third element. Then it applies the previous result and the fourth element. It continues like this until a single result remains.\n",
    "\n",
    "For example, imagine that the reducer function is the following add() function that adds two numbers:\n",
    "\n",
    "```\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "```\n",
    "\n",
    "Then **functools.reduce(add, [5, 7, 3, 9, 4, 6])** will add all values in the provided list by applying the **add()** function between the previous result and the next number in the list:\n",
    "\n",
    "```\n",
    "import functools\n",
    "print(functools.reduce(add, [5, 7, 3, 9, 4, 6]))\n",
    "```\n",
    "\n",
    "### We've provided you with the values list from the previous screen.\n",
    "\n",
    "1. Import the functools module.\n",
    "\n",
    "2. Use the functools.reduce() function to calculate the maximum value of values. Assign the result to a variable named max_value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b918f47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [1, 4, 5, 2, 7, 21,     \\\n",
    "          31, 41, 3, 40, 5, 14,  \\\n",
    "          9, 32, 12, 18, 1, 30,  \\\n",
    "          6, 19, 23, 35, 12, 13, \\\n",
    "          0, 12, 42, 41, 11, 9]\n",
    "\n",
    "# Write code here\n",
    "\n",
    "import functools\n",
    "max_value = functools.reduce(max, values)\n",
    "max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d741f",
   "metadata": {},
   "source": [
    "### We now have everything we need to implement our own MapReduce framework. Let's review the steps and determine which function we should use for each step.\n",
    "\n",
    "Let's begin where we want to calculate the maximum value of a list of numbers. The workflow is the following:\n",
    "\n",
    "The list of numbers is in a variable named data. We've provided the variable num_processes with the number of processes to use.\n",
    "\n",
    "1. Divide the data into chunks using the make_chunks() function.\n",
    "2. Use the Pool.map() method with the max function to calculate the maximum value of each chunk.\n",
    "3. Use the functools.reduce() function with the max function to combine the results of all chunks and identify the maximum value.\n",
    "\n",
    "The list of numbers is in a variable named **data**. We've provided the variable **num_processes** with the number of processes to use.\n",
    "\n",
    "1. Use the **make_chunks()** function to break the **data** into **num_processes** chunks. Assign the result to a variable named **chunks**.\n",
    "\n",
    "2. Use a context manager to create a **Pool** with **num_processes** processes. Use it with the name **pool**.\n",
    "\n",
    "3. Use the **Pool.map()** method to apply the **max()** built-in function to **chunks**. Assign the result to a variable named **chunk_results**.\n",
    "\n",
    "4. Use the **functools.reduce()** function to apply the **max()** built-in function to **chunk_results**. Assign the result to a variable named **overall_result**.\n",
    "\n",
    "5. Inspect the value of overall_result to ensure it is the maximum value of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e549acf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 4, 5, 2, 7, 21,     \\\n",
    "        31, 41, 3, 40, 5, 14,  \\\n",
    "        9, 32, 12, 18, 1, 30,  \\\n",
    "        6, 19, 23, 35, 12, 13, \\\n",
    "        0, 12, 42, 41, 11, 9]\n",
    "\n",
    "num_processes = 5\n",
    "\n",
    "# Write code here\n",
    "chunks = make_chunks(data, num_processes)\n",
    "\n",
    "with Pool(num_processes) as pool:\n",
    "    chunk_results = pool.map(max, chunks)\n",
    "\n",
    "overall_result = functools.reduce(max, chunk_results)\n",
    "overall_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343b295",
   "metadata": {},
   "source": [
    "We're going to finalize our work and create a map_reduce() function that we can use to apply MapReduce to any dataset. Here's the information a user needs to provide:\n",
    "\n",
    "1. The dataset itself.\n",
    "2. The number of processes to use (in other words, the number of chunks).\n",
    "3. The function that applies to each chunk. We call this the \"mapper function\".\n",
    "4. The function that combines the results of each chunk. We call this the \"reducer function\".\n",
    "\n",
    "The list of numbers is in a variable named data. We've provided the map_reduce() function that we developed in this lesson.\n",
    "\n",
    "This means that the map_reduce() function will have the following signature:\n",
    "\n",
    "```\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    # Implementation goes here\n",
    "```\n",
    "\n",
    "The implementation will be very similar to what we did in the previous cell. The difference is that instead of applying the max() built-in function, we will apply the provided mapper and reducer functions.\n",
    "\n",
    "The first step is to break the data into num_processes chunks:\n",
    "\n",
    "```\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "```\n",
    "\n",
    "Then we need to create a pool and use the Pool.map() method to apply the mapper function to each chunk of data:\n",
    "\n",
    "```\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with Pool(num_processes) as pool:\n",
    "        chunk_results = pool.map(mapper, chunks)\n",
    "```\n",
    "\n",
    "Finally, we use the functools.reduce() function to combine the chunk_results into a global result and return it:\n",
    "\n",
    "```\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with Pool(num_processes) as pool:\n",
    "        chunk_results = pool.map(mapper, chunks)\n",
    "    return functools.reduce(reducer, chunk_results)\n",
    "```\n",
    "The list of numbers is in a variable named data. We've provided the map_reduce() function that we developed in this lesson.\n",
    "* Use the map_reduce() function to calculate the maximum value of data. Use 4 processes. As before, the mapper and reducer functions are both the max() built-in function.\n",
    "\n",
    "* Assign the result from the previous step to a variable named max_value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0d823a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n",
    "data = [1, 4, 5, 2, 7, 21,     \\\n",
    "        31, 41, 3, 40, 5, 14,  \\\n",
    "        9, 32, 12, 18, 1, 30,  \\\n",
    "        6, 19, 23, 35, 12, 13, \\\n",
    "        0, 12, 43, 41, 11, 9]\n",
    "\n",
    "def make_chunks(data, num_chunks):\n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with Pool(num_processes) as pool:\n",
    "        chunk_results = pool.map(mapper, chunks)\n",
    "    return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "# Write code here\n",
    "max_value = map_reduce(data, 4, max, max)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f35b0c",
   "metadata": {},
   "source": [
    "## We've read the data into the job_postings and skills DataFrame. Your goal is to implement the mapper() and reducer() function and apply the map_reduce() function. This time, we'll break the skills DataFrame into chunks.\n",
    "\n",
    "1. Define a function named mapper() with a single argument named skill_chunk.\n",
    "\n",
    "2. Implement the mapper() function by doing the following:\n",
    "    1. Initializing an empty dictionary named frequency.\n",
    "    2. For each skill_name in skill_chunk[\"Name\"], count the number of occurrences of skill_name in the job_postings DataFrame (note that the job_postings DataFrame is available from the outside and not passed as arguments).\n",
    "    3. Assign the result to frequency[skill_name].\n",
    "    4. After the for loop, return the frequency dictionary.\n",
    "\n",
    "3. Define a function named reducer() with two arguments freq_chunk1 and freq_chunk2. These arguments will be the frequency tables of two chunks. This function should merge the results.\n",
    "\n",
    "4. Implement the reducer() function by using the dict.update() method to merge freq_chunk1 and freq_chunk2 into a single dictionary. Return the merged result.\n",
    "\n",
    "5. Call the map_reduce() with arguments: skills, 4, mapper, and reducer. Assign the result to skill_freq.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd0bcf55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n",
    "import pandas as pd\n",
    "job_postings = pd.read_csv(\"DataEngineer.csv\")\n",
    "job_postings[\"Job Description\"] = job_postings[\"Job Description\"].str.lower()\n",
    "skills = pd.read_csv(\"Skills.csv\")\n",
    "\n",
    "def mapper(skill_chunk):\n",
    "    frequency = {}\n",
    "    for skill_name in skill_chunk[\"Name\"]:\n",
    "        frequency[skill_name] = job_postings[\"Job Description\"].str.count(skill_name).sum()\n",
    "    return frequency\n",
    "\n",
    "def reducer(freq_chunk1, freq_chunk2):\n",
    "    freq_chunk1.update(freq_chunk2)\n",
    "    return freq_chunk1\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with Pool(num_processes) as pool:\n",
    "        chunk_results = pool.map(mapper, chunks)\n",
    "    return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#skill_freq = map_reduce(skills, 4, mapper, reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "947ed0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "job_postings = pd.read_csv(\"DataEngineer.csv\")\n",
    "job_postings[\"Job Description\"] = job_postings[\"Job Description\"].str.lower()\n",
    "skills = pd.read_csv(\"Skills.csv\")\n",
    "\n",
    "# Write code here\n",
    "def mapper(jobs_chunk):\n",
    "    frequency = {}\n",
    "    for skill_name in skills[\"Name\"]:\n",
    "        frequency[skill_name] = jobs_chunk[\"Job Description\"].str.count(skill_name).sum()\n",
    "    return frequency\n",
    "\n",
    "def reducer(freq_chunk1, freq_chunk2):\n",
    "    merged = {}\n",
    "    for skill in freq_chunk1:\n",
    "        merged[skill] = freq_chunk1[skill] + freq_chunk2[skill]\n",
    "    return merged\n",
    "\n",
    "#skill_freq = map_reduce(job_postings, 4, mapper, reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd40fdf",
   "metadata": {},
   "source": [
    "### In the Introduction to MapReduce lesson, we learned about MapReduce and implemented our own MapReduce framework. Here is the workflow for using it:\n",
    "\n",
    "1. Implement a mapper function that takes as input a single chunk of data. This function should process the data chunk and return the processed data.\n",
    "\n",
    "2. Implement a reducer function that takes as input two results from the mapper function and combines them into a single result.\n",
    "\n",
    "3. Provide both functions to the MapReduce framework. The data will then automatically divide into chunks and process in parallel.\n",
    "\n",
    "\n",
    "The map_reduce() function from the Introduction to MapReduce lesson will be available on all screens. It takes input four arguments as input:\n",
    "\n",
    "1. The data\n",
    "2. The number of processes\n",
    "3. The mapper function\n",
    "4. The reducer function\n",
    "\n",
    "### We've provided a list of numbers values.\n",
    "\n",
    "1. Use the map_reduce() function to calculate the minimum value in the values list using 4 processes. Assign the result to a variable name min_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a2ad93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with Pool(num_processes) as pool:\n",
    "        chunk_results = pool.map(mapper, chunks)\n",
    "    return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "values = [98, 63, 55, 80, 45, 51, 91, 64, 65, 48, 48, 92, 76, 99, 57, 42, 79, 61, 63, 49]\n",
    "\n",
    "min_value = map_reduce(values, 4, min, min)\n",
    "print(min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c87c3d",
   "metadata": {},
   "source": [
    "### We'll use MapReduce to calculate the length of the longest word in the English language.\n",
    "\n",
    "#### We've read the word_list.txt file into a list named words.\n",
    "\n",
    "1. Define a function named map_max_length with a single argument named words_chunk.\n",
    "\n",
    "2. Implement the map_max_length() function so that it returns the length of the longest string in the words_chunk list.\n",
    "\n",
    "3. Use the map_reduce() function to calculate the length of the longest string in words using 4 processes. Assign the result to a variable named max_len.\n",
    "\n",
    "4. Print the value of max_len to see the length of the longest English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e87663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# from multiprocessing import Pool\n",
    "# import functools\n",
    "\n",
    "# with open(\"english_words.txt\") as f:\n",
    "#     words = [word.strip() for word in f.readlines()]\n",
    "\n",
    "# def map_max_length(words_chunk):\n",
    "#     return max([len(word) for word in words_chunk])\n",
    "\n",
    "# def make_chunks(data, num_chunks):\n",
    "#     chunk_size = math.ceil(len(data) / num_chunks)\n",
    "#     return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __spec__ = None\n",
    "#     def map_reduce(data, num_processes, mapper, reducer):\n",
    "#         chunks = make_chunks(data, num_processes)\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             chunk_results = pool.map(mapper, chunks)\n",
    "#         return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#     max_len = map_reduce(words, 4, map_max_length, max)\n",
    "#     print(max_len)\n",
    "\n",
    "%run longest_english_word.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89546828",
   "metadata": {},
   "source": [
    "### Let's write another MapReduce program that calculates the actual word instead of the length.\n",
    "\n",
    "To calculate the longest string in a list, we can actually use the max() built-in function. From the documentation, we see that the max() function accepts an argument called key. We can use this argument to specify how to compare the values in the list.\n",
    "\n",
    "The key argument should be a function that returns a numeric value. This function will then apply to each element. The one with the highest value will return. By using the len() built-in function as the key arguments, we get the longest string:\n",
    "\n",
    "```\n",
    "max_str = max([\"science\", \"programming\", \"database\", \"python\"], key=len)\n",
    "print(max_str)\n",
    "\n",
    "programming\n",
    "```\n",
    "\n",
    "### We've read the word_list.txt file into a list named words.\n",
    "\n",
    "1. Define a function named map_max_len_str with a single argument named words_chunk.\n",
    "\n",
    "2. Implement the map_max_len_str() function so that it returns the longest string in words_chunk. You can use the max() built-in function with the key keyword argument to implement it.\n",
    "\n",
    "3. Define a function reduce_max_len_str with two arguments word1 and word2.\n",
    "\n",
    "4. Implement the reduce_max_len_str() function so that it returns word1 if it is longer than or equal to the length of word2. Otherwise, return word2.\n",
    "\n",
    "5. Use the map_reduce() function to calculate the longest string in words using 4 processes. Assign the result to available named max_len_str.\n",
    "\n",
    "6. Print the value of max_len_str to see the longest English word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79ce48e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromophotolithograph\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# from multiprocessing import Pool\n",
    "# import functools\n",
    "\n",
    "# with open(\"english_words.txt\") as f:\n",
    "#     words = [word.strip() for word in f.readlines()]\n",
    "    \n",
    "# def map_max_len_str(words_chunk):\n",
    "#     return max(words_chunk, key=len) \n",
    "\n",
    "# def reduce_max_len_str(word1, word2):\n",
    "#     return map_max_len_str([word1, word2])\n",
    "\n",
    "# def make_chunks(data, num_chunks):\n",
    "#     chunk_size = math.ceil(len(data) / num_chunks)\n",
    "#     return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __spec__ = None\n",
    "#     def map_reduce(data, num_processes, mapper, reducer):\n",
    "#         chunks = make_chunks(data, num_processes)\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             chunk_results = pool.map(mapper, chunks)\n",
    "#         return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#     max_len_str = map_reduce(words, 4, map_max_len_str, reduce_max_len_str)\n",
    "#     print(max_len_str)\n",
    "    \n",
    "%run longest_english_string.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7253ec",
   "metadata": {},
   "source": [
    "### How can we use MapReduce to find out if a given word is in the list of English words we're using in this lesson?\n",
    "\n",
    "For each chunk of words, we can use the in operator to check whether the word is in the chunk. This means that the mapper function will map each chunk into a Boolean value: True if the word is in the chunks and False otherwise.\n",
    "\n",
    "Then, the reducer function needs to combine these results in a way that tells us if the word is in any of the chunks. We can do this by using the or Boolean operator to combine the results of the two chunks. Given any number of Boolean values, the logical or of all those values will be true if at least one of them is true.\n",
    "\n",
    "### We've read the word_list.txt file into a list named words. The target string (the one we're looking for) is in a variable named target. Note that this doesn't pass as arguments to neither the mapper nor the reducer. They can access it because it is declared outside.\n",
    "\n",
    "1. Define a function named map_contains with a single argument named words_chunk.\n",
    "\n",
    "2. Implement the map_contains() function so that it returns True if the words_chunk contains the string in target.\n",
    "\n",
    "3. Define a function reduce_contains with two Boolean arguments contains1 and contains2.\n",
    "\n",
    "4. Implement the reduce_contains function so that it returns True if at least one of contains1 or contains2 is true. Otherwise, the function should return False.\n",
    "\n",
    "5. Use the map_reduce() function to check whether the words list contains the word pneumonoultramicroscopicsilicovolcanoconiosis, which is stored in the variable target. Assign the result to a variable named is_contained.\n",
    "\n",
    "6. Print the value of is_contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ca876d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# from multiprocessing import Pool\n",
    "# import functools\n",
    "\n",
    "# with open(\"english_words.txt\") as f:\n",
    "#     words = [word.strip() for word in f.readlines()]\n",
    "\n",
    "# target = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\n",
    "\n",
    "# def map_contains(words_chunk):\n",
    "#     return target in words_chunk\n",
    "    \n",
    "# def reduce_contains(contains1, contains2):\n",
    "#     return contains1 or contains2\n",
    "\n",
    "# def make_chunks(data, num_chunks):\n",
    "#     chunk_size = math.ceil(len(data) / num_chunks)\n",
    "#     return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __spec__ = None\n",
    "#     def map_reduce(data, num_processes, mapper, reducer):\n",
    "#         chunks = make_chunks(data, num_processes)\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             chunk_results = pool.map(mapper, chunks)\n",
    "#         return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#     is_contained = map_reduce(words, 4,map_contains, reduce_contains)\n",
    "#     print(is_contained)\n",
    "\n",
    "%run processing_data_w_map_reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11089a5b",
   "metadata": {},
   "source": [
    "Let's continue practicing MapReduce by counting the frequency of the characters throughout the entire list of words. The idea is to see which letters from the English alphabet appear most frequently. For example, we want to count how many times the letter a appears, how many times the letter b appears, and so on.\n",
    "\n",
    "If the list only had the words data and science then the result would be:\n",
    "\n",
    "```\n",
    "{\n",
    "    'd': 1, \n",
    "    'a': 2, \n",
    "    't': 1, \n",
    "    's': 1, \n",
    "    'c': 2, \n",
    "    'i': 1, \n",
    "    'e': 2, \n",
    "    'n': 1\n",
    "}\n",
    "```\n",
    "\n",
    "We're counting characters rather than words. The mapper function will create a frequency table for the provided chunk of words.\n",
    "\n",
    "Then, the reducer will take two frequency tables and merge them by adding together the counts of both letters. In this case, the two dictionaries will not necessarily have the same set of keys because some characters might not occur in one of the chunks.\n",
    "\n",
    "For example, if the result of one chunk is {'a': 3, 'b': 2} and the result of the other chunk is {'b': 2, 'c': 2} then the reduce result must be:\n",
    "\n",
    "```\n",
    "{\n",
    "    'a': 3,\n",
    "    'b': 4,\n",
    "    'c': 2\n",
    "}\n",
    "```\n",
    "\n",
    "### We've read the word_list.txt file into a list named words.\n",
    "\n",
    "1. Define a function named map_char_count with a single argument named words_chunk.\n",
    "\n",
    "2. Implement the map_char_count() function by following these steps:\n",
    "    1. Initialize an empty dictionary in a variable named char_freq.\n",
    "    2. Use a for loop over words_chunks with the variable word.\n",
    "    3. Inside, use a second for loop over word with variable c. This loop will iterate over all characters in word.\n",
    "    4. Use the in operator to check whether c is in char_freq. If it isn't, set char_freq[c] to 0.\n",
    "    5. Inside the second for loop but outside of the if statement, increment the value of char_freq[c] by 1.\n",
    "    6. At the end, outside of the two for loops, return the value of char_freq.\n",
    "\n",
    "3. Define a function named reduce_char_count with two arguments named freq1 and freq2. These will be two dictionaries with character frequencies of two chunks of words.\n",
    "\n",
    "4. Implement the reduce_char_count() function by following these steps:\n",
    "    1. Use a for loop to iterate over all keys in freq2 using variable c.\n",
    "    2. If c is in freq1, add freq2[c] to freq1[c].\n",
    "    3. Otherwise, set the value of freq1[c] to freq2[c].\n",
    "    4. Outside of the for loop, return the value of freq1.\n",
    "\n",
    "5. Use the map_reduce() function to build a character frequency table of the words list using 4 processes. Assign the result to a variable named char_freq.\n",
    "\n",
    "6. Print the value of char_freq and see which English character appears most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d08af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 71683, 'm': 26264, 'r': 61289, 'o': 59303, 'n': 55947, 'i': 73051, 'c': 38519, 'l': 47576, 'b': 15500, 't': 59007, 'e': 90720, 's': 50422, 'u': 30414, 'k': 5278, 'd': 24568, 'f': 10137, 'y': 18011, 'g': 18161, 'h': 21742, 'v': 8166, 'x': 2701, 'w': 5511, 'p': 27149, 'j': 1250, 'q': 1656, 'z': 2722}\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# from multiprocessing import Pool\n",
    "# import functools\n",
    "\n",
    "# with open(\"english_words.txt\") as f:\n",
    "#     words = [word.strip() for word in f.readlines()]\n",
    "    \n",
    "# def map_char_count(words_chunk):\n",
    "#     char_freq = {}\n",
    "#     for word in words_chunk:\n",
    "#         for c in word:\n",
    "#             if c not in char_freq:\n",
    "#                 char_freq[c] = 0\n",
    "#             char_freq[c] += 1\n",
    "#     return char_freq\n",
    "\n",
    "# def reduce_char_count(freq1, freq2):\n",
    "#     for c in freq2:\n",
    "#         if c in freq1:\n",
    "#             freq1[c] += freq2[c]\n",
    "#         else:\n",
    "#             freq1[c] = freq2[c]\n",
    "#     return freq1\n",
    "\n",
    "# def make_chunks(data, num_chunks):\n",
    "#     chunk_size = math.ceil(len(data) / num_chunks)\n",
    "#     return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __spec__ = None\n",
    "#     def map_reduce(data, num_processes, mapper, reducer):\n",
    "#         chunks = make_chunks(data, num_processes)\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             chunk_results = pool.map(mapper, chunks)\n",
    "#         return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#     char_freq = map_reduce(words, 4, map_char_count, reduce_char_count)\n",
    "#     print(char_freq)\n",
    "    \n",
    "%run char_freq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae9fb9",
   "metadata": {},
   "source": [
    "Now let's calculate the average length of English words.\n",
    "\n",
    "The average length is the sum of the word lengths divided by the number of words: \n",
    "\n",
    "```\n",
    "(4 + 9 + 11 + 7) =  31/4 = 7.75\n",
    "(4)\n",
    "```\n",
    "\n",
    "We might think to implement the mapper function by calculating the average of the given chunk. However, since the reducer function merges two results at a time, it is difficult to merge the chunk averages in a way that yields a global average.\n",
    "\n",
    "A simpler solution is to make mappers calculate the sum of the lengths of all words in that chunk divided by the total number of words in the entire word list. In this way, the reducer function can simply add the results to get the overall average.\n",
    "\n",
    "### We've read the word_list.txt file into a list named words.\n",
    "\n",
    "1. Define a function named map_average with a single argument named words_chunk.\n",
    "\n",
    "2. Implement the map_average() function with the following steps:\n",
    "    1. Calculate the sum of the lengths of all words in words_chunk.\n",
    "    2. Divide the result by the length of words. The words variable is available from the outside.\n",
    "\n",
    "3. Define a function named reduce_average with two arguments named res1 and res2.\n",
    "\n",
    "4. Implement the reduce_average() function so that it returns the value of res1 + res2.\n",
    "\n",
    "5. Use the map_reduce() function to calculate the average length of the words the words list using 4 processes. Assign the result to a variable named average_word_len.\n",
    "\n",
    "6. Print the value of average_word_len and see the average length of the English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74dfb93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.622276685612974\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# from multiprocessing import Pool\n",
    "# import functools\n",
    "\n",
    "# with open(\"english_words.txt\") as f:\n",
    "#     words = [word.strip() for word in f.readlines()]\n",
    "    \n",
    "# def map_average(words_chunk):\n",
    "#     length = 0\n",
    "#     for word in words_chunk:\n",
    "#         length += len(word)\n",
    "#     return length / len(words)\n",
    "\n",
    "# def reduce_average(res1, res2):\n",
    "#     return res1 + res2\n",
    "\n",
    "# def make_chunks(data, num_chunks):\n",
    "#     chunk_size = math.ceil(len(data) / num_chunks)\n",
    "#     return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __spec__ = None\n",
    "#     def map_reduce(data, num_processes, mapper, reducer):\n",
    "#         chunks = make_chunks(data, num_processes)\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             chunk_results = pool.map(mapper, chunks)\n",
    "#         return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#     average_word_len = map_reduce(words, 4, map_average, reduce_average)\n",
    "#     print(average_word_len)\n",
    "    \n",
    "%run avg_word_length.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71303814",
   "metadata": {},
   "source": [
    "### We are interested in words with characters that appear next to each other the least often. For example, consider the word science. The characters that occurs next to each other are as follows:\n",
    "\n",
    "```\n",
    "sc\n",
    "ci\n",
    "ie\n",
    "en\n",
    "nc\n",
    "ce\n",
    "```\n",
    "\n",
    "We can calculate them using a for loop, like so:\n",
    "```\n",
    "word = \"science\"\n",
    "for i in range(len(word) - 1):\n",
    "    seq = word[i] + word[i + 1]\n",
    "    print(seq)\n",
    "```\n",
    "\n",
    "The goal is to find which pairs of characters occur next to each other in only one word.\n",
    "\n",
    "For this, we will use MapReduce to build a frequency table of all pairs of consecutive characters in all words of the dataset. The pairs that occurs only once will be the ones with a frequency equal to one.\n",
    "\n",
    "The mapper function will build the frequency table of each chunk and the reducer function will merge the frequency table in the same way as we did before.\n",
    "\n",
    "### We've read the word_list.txt file into a list named words.\n",
    "\n",
    "1. Define a function named map_adjacent with a single argument named words_chunk.\n",
    "\n",
    "2. Implement the map_adjacent() function so that it calculates a frequency table of the pairs of adjacent characters of all words in words_chunk.\n",
    "\n",
    "3. Define a function named reduce_adjacent with two arguments named freq1 and freq2.\n",
    "\n",
    "4. Implement the reduce_adjacent() function so that it returns a dictionary that results from merging freq1 and freq2.\n",
    "\n",
    "5. Use the map_reduce() function to calculate all pairs of consecutive characters that appear a single time in the words list using 4 processes. Assign the result to a variable named pair_freq.\n",
    "\n",
    "6. Use a for loop to create a list unique_pairs with all keys from pair_freq whose values is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f7aabaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zt', 'zp', 'xk', 'wq', 'vn', 'gv', 'cg', 'zg', 'fj', 'cf', 'jr', 'vh', 'jh', 'gj', 'jy', 'pv', 'cw', 'zq', 'zm', 'yq', 'rx', 'zv', 'vd', 'vl']\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# from multiprocessing import Pool\n",
    "# import functools\n",
    "\n",
    "# with open(\"english_words.txt\") as f:\n",
    "#     words = [word.strip() for word in f.readlines()]\n",
    "    \n",
    "# def map_adjacent(words_chunk):\n",
    "#     adj_freq = {}\n",
    "#     for word in words_chunk:\n",
    "#         for i in range(len(word) - 1):\n",
    "#             seq = word[i] + word[i + 1]\n",
    "#             if seq not in adj_freq:\n",
    "#                 adj_freq[seq] = 0\n",
    "#             adj_freq[seq] += 1\n",
    "#     return adj_freq\n",
    "\n",
    "\n",
    "# def reduce_adjacent(freq1, freq2):\n",
    "#     for seq in freq2:\n",
    "#         if seq in freq1:\n",
    "#             freq1[seq] += freq2[seq]\n",
    "#         else:\n",
    "#             freq1[seq] = freq2[seq]\n",
    "#     return freq1\n",
    "\n",
    "# def make_chunks(data, num_chunks):\n",
    "#     chunk_size = math.ceil(len(data) / num_chunks)\n",
    "#     return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __spec__ = None\n",
    "#     def map_reduce(data, num_processes, mapper, reducer):\n",
    "#         chunks = make_chunks(data, num_processes)\n",
    "#         with Pool(num_processes) as pool:\n",
    "#             chunk_results = pool.map(mapper, chunks)\n",
    "#         return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "#     pair_freq = map_reduce(words, 4, map_adjacent, reduce_adjacent)\n",
    "#     unique_pairs = [seq for seq in pair_freq if pair_freq[seq] == 1]\n",
    "#     print(unique_pairs)\n",
    "    \n",
    "%run rare_adj_chars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcebbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
